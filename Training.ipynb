{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d238262",
   "metadata": {},
   "source": [
    "### Unsupervised GRU-VAE training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c01e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np # For standard deviation calculation\n",
    "from modbus import ModbusDataset,ModbusFlowStream\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import os \n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import random\n",
    "from utils import load_scalers\n",
    "from random import SystemRandom\n",
    "\n",
    "def compute_threshold(mse_values):\n",
    "    \"\"\"\n",
    "    Computes the anomaly detection threshold (for marking sample as Intrusion if the IS was greater )\n",
    "    based on the mean and standard deviation of Mean Squared Error (MSE) values.\n",
    "    Formula: thr = mean(MSE) + std(MSE)\n",
    "\n",
    "    Args:\n",
    "        mse_values (torch.Tensor or list/np.array): A tensor or list of MSE values\n",
    "                                                    obtained from the validation set.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated threshold.\n",
    "    \"\"\"\n",
    "    if not isinstance(mse_values, torch.Tensor):\n",
    "        mse_values = torch.tensor(mse_values, dtype=torch.float32)\n",
    "\n",
    "    if mse_values.numel() == 0:\n",
    "        return 0.0 \n",
    "    mean_mse = torch.mean(mse_values)\n",
    "    std_mse = torch.std(mse_values)\n",
    "\n",
    "    threshold = mean_mse + std_mse\n",
    "    return threshold.item() \n",
    "\n",
    "def vae_loss_function(recon_x, x, mu, logvar,beta =1):\n",
    "    \"\"\"\n",
    "    VAE loss function.\n",
    "    \"\"\"\n",
    "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return (BCE + beta*KLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9478520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The CIC Modbus Dataset contains network (pcap) captures and attack logs from a simulated substation network.\n",
      "                The dataset is categorized into two groups: an attack dataset and a benign dataset\n",
      "                The attack dataset includes network traffic captures that simulate various types of Modbus protocol attacks in a substation environment.\n",
      "                The attacks are reconnaissance, query flooding, loading payloads, delay response, modify length parameters, false data injection, stacking Modbus frames, brute force write and baseline replay.\n",
      "                These attacks are based of some techniques in the MITRE ICS ATT&CK framework.\n",
      "                On the other hand, the benign dataset consists of normal network traffic captures representing legitimate Modbus communication within the substation network.\n",
      "                The purpose of this dataset is to facilitate research, analysis, and development of intrusion detection systems, anomaly detection algorithms and other security mechanisms for substation networks using the Modbus protocol.\n",
      "                https://www.unb.ca/cic/datasets/modbus-2023.html\n",
      "                In my custom PyTorch Dataset class,\n",
      "                I utilize the Enhanced CICflowMeter and the Attack logs correlation to extract and label sequential data flows,\n",
      "                preparing them for batch processing with the DataLoader, which is crucial for AI model training.\n",
      "                https://github.com/hamid-rd/FLBased-ICS-NIDS/tree/main\n",
      "\n",
      "                \n",
      "csv files  in the dataset directory founded with the filter:  ready\n",
      "{\n",
      "    \"total_dataset_num\": 170,\n",
      "    \"benign_dataset_num\": 62,\n",
      "    \"attack_dataset_num\": {\n",
      "        \"total_num\": 108,\n",
      "        \"external_num\": 8,\n",
      "        \"compromised-ied_num\": 43,\n",
      "        \"compromised-scada_num\": 57\n",
      "    },\n",
      "    \"attack_logs_num\": {\n",
      "        \"total_num\": 0,\n",
      "        \"external_num\": [],\n",
      "        \"compromised-ied_num\": 0,\n",
      "        \"compromised-scada_num\": 0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset_directory = \"./dataset\" # change this to the folder contain benign and attack subdirs\n",
    "modbus = ModbusDataset(dataset_directory,\"ready\")\n",
    "modbus.summary_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "510ea189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# AutoEncoder (AE)\n",
    "class AE(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder: (89-64-32)\n",
    "    Decoder: (32-64-89)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(89, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 89),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4698b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded scalers for 'network-wide'\n",
      "4779\n"
     ]
    }
   ],
   "source": [
    "loaded_scalers = load_scalers(\"fitted_scalers\")\n",
    "\n",
    "AE_dataset=ModbusFlowStream( \n",
    "    shuffle=True,chunk_size=1,batch_size=64,csv_files=modbus.dataset[\"benign_dataset_dir\"][0:2],scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=1\n",
    ")\n",
    "AE_dataloader=DataLoader(AE_dataset,batch_size=1,shuffle=False)\n",
    "csv_files=modbus.dataset[\"benign_dataset_dir\"][0:5]\n",
    "print(len(AE_dataloader))\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "AE_model = AE().to(device)\n",
    "lr = 0.01\n",
    "wd= 1e-4\n",
    "shuffle_files =True\n",
    "AE_optimizer = optim.Adam(AE_model.parameters(), lr=lr, weight_decay=wd)\n",
    "criterion = nn.MSELoss(reduction='sum').to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d2e0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 15.620971202850342 Epoch 0, Train Loss: 31.216262530023318\n",
      "time 16.269652605056763 Epoch 1, Train Loss: 25.86237255363041\n",
      "time 17.673813819885254 Epoch 2, Train Loss: 9.607960329345266\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(3):\n",
    "    time_1 = time.time()\n",
    "    train_loss = 0\n",
    "    AE_model.train()\n",
    "    if shuffle_files:\n",
    "        sys_rand = SystemRandom()\n",
    "        sys_rand.shuffle(AE_dataset.file_order_indices)\n",
    "    for sequences, _ in AE_dataloader:\n",
    "        sequences=sequences.squeeze().to(device)\n",
    "        AE_optimizer.zero_grad()\n",
    "        recon = AE_model(sequences)\n",
    "        loss = criterion(recon, sequences) \n",
    "        loss.backward()\n",
    "        AE_optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    print(\"time\",time.time()-time_1,f\"Epoch {epoch}, Train Loss: {train_loss / len(AE_dataloader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Variational AutoEncoder (VAE)\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder: (89-64-64-32 for mu and log_var)\n",
    "    Decoder: (32-64-64-89)\n",
    "    return x_recon, mu, logvar\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(89, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64, 32)\n",
    "        self.fc_logvar = nn.Linear(64, 32)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 89),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "\n",
    "\n",
    "# #Example of iterating\n",
    "# GRU_dataset=ModbusFlowStream(\n",
    "#     shuffle=False,chunk_size=1,batch_size=64,csv_files=modbus.dataset[\"benign_dataset_dir\"][0:2],scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=30\n",
    "# )\n",
    "\n",
    "# GRU_dataloader=DataLoader(GRU_dataset,batch_size=1,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6081d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 20.744476318359375 Epoch 0, Train Loss: 143.65623077564993\n",
      "time 19.662742614746094 Epoch 1, Train Loss: 110.98179194946673\n",
      "time 19.732792854309082 Epoch 2, Train Loss: 94.82658667383795\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "wd= 1e-4\n",
    "\n",
    "VAE_model = VAE().to(device=device)\n",
    "VAE_optimizer = optim.Adam(VAE_model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "AE_dataset=ModbusFlowStream( \n",
    "    shuffle=True,chunk_size=1,batch_size=64,csv_files=modbus.dataset[\"benign_dataset_dir\"][0:2],scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=1\n",
    ")\n",
    "AE_dataloader=DataLoader(AE_dataset,batch_size=1,shuffle=False)\n",
    "\n",
    "for epoch in range(3):\n",
    "    time_1 = time.time()\n",
    "    train_loss = 0\n",
    "    AE_model.train()\n",
    "    if shuffle_files:\n",
    "        sys_rand = SystemRandom()\n",
    "        sys_rand.shuffle(AE_dataset.file_order_indices)\n",
    "    for sequences, _ in AE_dataloader:\n",
    "        sequences = sequences.squeeze().to(device)\n",
    "        VAE_optimizer.zero_grad()\n",
    "        recon, mu, logvar = VAE_model(sequences)\n",
    "        loss = vae_loss_function(recon, sequences, mu, logvar)\n",
    "        loss.backward()\n",
    "        VAE_optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(\"time\",time.time()-time_1,f\"Epoch {epoch}, Train Loss: {train_loss / len(AE_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48a5bd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AAE_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Encoder(Generator):(89-16-4-2)\n",
    "        \"\"\"\n",
    "        super(AAE_Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(89, 16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(16, 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(4, 2))\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "class AAE_Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AAE_Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(4, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 89),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "class AAE_Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AAE_Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(4, 2), # Output for binary classification (real/fake)\n",
    "            nn.Sigmoid()\n",
    "        )    \n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ea5785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "aae_encoder = AAE_Encoder()\n",
    "aae_decoder = AAE_Decoder()\n",
    "aae_discriminator = AAE_Discriminator()\n",
    "aae_encoder.to(device)\n",
    "aae_decoder.to(device)\n",
    "aae_discriminator.to(device)\n",
    "optimizer_G = optim.Adam(list(aae_encoder.parameters()) + list(aae_decoder.parameters()), lr=1e-4)\n",
    "optimizer_D = optim.Adam(aae_discriminator.parameters(), lr=1e-4)\n",
    "adversarial_loss = nn.BCELoss(reduction=\"sum\")\n",
    "reconstruction_loss = nn.MSELoss(reduction=\"sum\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f371a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/5] [G loss: 6.953534] [D loss: 38.366356]\n",
      "[Epoch 1/5] [G loss: 2.036054] [D loss: 27.961067]\n",
      "[Epoch 2/5] [G loss: 4.855754] [D loss: 52.275917]\n",
      "[Epoch 3/5] [G loss: 1.519117] [D loss: 44.207191]\n",
      "[Epoch 4/5] [G loss: 2.031374] [D loss: 37.788727]\n"
     ]
    }
   ],
   "source": [
    "num_epochs=5\n",
    "for epoch in range(num_epochs):\n",
    "    aae_encoder.train()\n",
    "    aae_decoder.train()\n",
    "    aae_discriminator.train()\n",
    "    if shuffle_files:\n",
    "        sys_rand = SystemRandom()\n",
    "        sys_rand.shuffle(AE_dataset.file_order_indices)\n",
    "    for sequences,_ in AE_dataloader:\n",
    "        sequences=sequences.squeeze().to(device)\n",
    "        # 1) reconstruction + generator loss\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_z = aae_encoder(sequences)\n",
    "        decoded_seq = aae_decoder(fake_z)\n",
    "        G_loss = 0.001*adversarial_loss(aae_discriminator(fake_z),  torch.ones(sequences.size(0), 2,device=device)) + 0.999*reconstruction_loss(decoded_seq, sequences)\n",
    "        G_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        # 2) discriminator loss\n",
    "        optimizer_D.zero_grad()\n",
    "        real_loss = adversarial_loss(aae_discriminator(torch.randn(sequences.size(0), 2, device=device)),  torch.ones(sequences.size(0), 2,device=device))\n",
    "        fake_loss = adversarial_loss(aae_discriminator(fake_z.detach()),  torch.zeros(sequences.size(0), 2,device=device))\n",
    "        D_loss = 0.5*(real_loss + fake_loss)\n",
    "        D_loss.backward()\n",
    "        optimizer_D.step()\n",
    "    # print loss\n",
    "    print(\n",
    "            \"[Epoch %d/%d] [G loss: %f] [D loss: %f]\"\n",
    "            % (epoch, num_epochs, G_loss.item(), D_loss.item())\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3698c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU-VAE\n",
    "class GRUVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Recurrent Unit : num_layers=2, hidden_size=256, dropout=0.01,window size (seq_len)= 40\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=89, hidden_dim=256, latent_dim=32, num_layers=2, dropout=0.01):\n",
    "        super(GRUVAE, self).__init__()\n",
    "        self.encoder_gru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_z_to_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_gru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, input_dim=89]\n",
    "        _, hidden = self.encoder_gru(x) \n",
    "        h = hidden[-1]  # [batch_size, hidden_dim]\n",
    "        mu = self.fc_mu(h)  \n",
    "        logvar = self.fc_logvar(h)  \n",
    "        z = self.reparameterize(mu, logvar)  # [batch_size, latent_dim]\n",
    "        # repeat and feed latent z as input trick\n",
    "        h0 = self.fc_z_to_hidden(z).unsqueeze(0).repeat(self.encoder_gru.num_layers, 1, 1)  # [num_layers, batch_size, hidden_dim]\n",
    "        # Initialize decoder input with zeros \n",
    "        decoder_input = torch.zeros_like(x)\n",
    "        output, _ = self.decoder_gru(decoder_input, h0)  # [batch_size, seq_len, hidden_dim]\n",
    "        x_recon = self.fc_out(output)  # [batch_size, seq_len, input_dim]\n",
    "        return x_recon, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799e51d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded scalers for 'network-wide'\n"
     ]
    }
   ],
   "source": [
    "loaded_scalers = load_scalers(\"fitted_scalers\")\n",
    "RNN_dataset=ModbusFlowStream( \n",
    "    shuffle=False,chunk_size=1,batch_size=64,csv_files=modbus.dataset[\"benign_dataset_dir\"][0:2],\n",
    "    scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=30\n",
    ")\n",
    "RNN_dataloadder=DataLoader(RNN_dataset,batch_size=1,shuffle=False)\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "GRU_VAE_model = GRUVAE().to(device)\n",
    "lr = 0.01\n",
    "wd= 1e-4\n",
    "shuffle_files =True\n",
    "GRU_VAE_optimizer = optim.Adam(GRU_VAE_model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 91.54682207107544 Epoch 0, Train Loss: 15274.382437467275\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m recon, mu, logvar \u001b[38;5;241m=\u001b[39m GRU_VAE_model(sequences)\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m vae_loss_function(recon, sequences, mu, logvar)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m GRU_VAE_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/labeling/Project1404/FLBased-ICS-NIDS/vnv/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/labeling/Project1404/FLBased-ICS-NIDS/vnv/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/labeling/Project1404/FLBased-ICS-NIDS/vnv/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    time_1 = time.time()\n",
    "    train_loss = 0\n",
    "    GRU_VAE_model.train()\n",
    "    if shuffle_files:\n",
    "        sys_rand = SystemRandom()\n",
    "        sys_rand.shuffle(RNN_dataset.file_order_indices)\n",
    "    for sequences, _ in RNN_dataloadder:\n",
    "        sequences = sequences.squeeze().to(device)\n",
    "        GRU_VAE_optimizer.zero_grad()\n",
    "        recon, mu, logvar = GRU_VAE_model(sequences)\n",
    "        loss = vae_loss_function(recon, sequences, mu, logvar)/sequences.size(0)\n",
    "        loss.backward()\n",
    "        GRU_VAE_optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(\"time\",time.time()-time_1,f\"Epoch {epoch}, Train Loss: {train_loss / len(RNN_dataloadder)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
