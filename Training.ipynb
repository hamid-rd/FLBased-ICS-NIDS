{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d238262",
   "metadata": {},
   "source": [
    "### Unsupervised GRU-VAE training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c01e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np # For standard deviation calculation\n",
    "from modbus import ModbusDataset,ModbusFlowStream\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix,recall_score\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import os \n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import random\n",
    "from utils import load_scalers\n",
    "from random import SystemRandom\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def compute_threshold(mse_values):\n",
    "    \"\"\"\n",
    "    Computes the anomaly detection threshold (for marking sample as Intrusion if the IS was greater )\n",
    "    based on the mean and standard deviation of Mean Squared Error (MSE) values.\n",
    "    Formula: thr = mean(MSE) + std(MSE)\n",
    "\n",
    "    Args:\n",
    "        mse_values (torch.Tensor or list/np.array): A tensor or list of MSE values\n",
    "                                                    obtained from the validation set.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated threshold.\n",
    "    \"\"\"\n",
    "    if not isinstance(mse_values, torch.Tensor):\n",
    "        mse_values = torch.tensor(mse_values, dtype=torch.float32)\n",
    "\n",
    "    if mse_values.numel() == 0:\n",
    "        return 0.0 \n",
    "    mean_mse = torch.mean(mse_values)\n",
    "    std_mse = torch.std(mse_values)\n",
    "\n",
    "    threshold = mean_mse + std_mse\n",
    "    return threshold.item() \n",
    "\n",
    "def vae_loss_function(recon_x, x, mu, logvar,beta =1):\n",
    "    \"\"\"\n",
    "    VAE loss function.\n",
    "    \"\"\"\n",
    "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return (BCE + beta*KLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9478520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The CIC Modbus Dataset contains network (pcap) captures and attack logs from a simulated substation network.\n",
      "                The dataset is categorized into two groups: an attack dataset and a benign dataset\n",
      "                The attack dataset includes network traffic captures that simulate various types of Modbus protocol attacks in a substation environment.\n",
      "                The attacks are reconnaissance, query flooding, loading payloads, delay response, modify length parameters, false data injection, stacking Modbus frames, brute force write and baseline replay.\n",
      "                These attacks are based of some techniques in the MITRE ICS ATT&CK framework.\n",
      "                On the other hand, the benign dataset consists of normal network traffic captures representing legitimate Modbus communication within the substation network.\n",
      "                The purpose of this dataset is to facilitate research, analysis, and development of intrusion detection systems, anomaly detection algorithms and other security mechanisms for substation networks using the Modbus protocol.\n",
      "                https://www.unb.ca/cic/datasets/modbus-2023.html\n",
      "                In my custom PyTorch Dataset class,\n",
      "                I utilize the Enhanced CICflowMeter and the Attack logs correlation to extract and label sequential data flows,\n",
      "                preparing them for batch processing with the DataLoader, which is crucial for AI model training.\n",
      "                https://github.com/hamid-rd/FLBased-ICS-NIDS/tree/main\n",
      "\n",
      "                \n",
      "csv files  in the dataset directory founded with the filter:  ready\n",
      "{\n",
      "    \"total_dataset_num\": 170,\n",
      "    \"benign_dataset_num\": 62,\n",
      "    \"attack_dataset_num\": {\n",
      "        \"total_num\": 108,\n",
      "        \"external_num\": 8,\n",
      "        \"compromised-ied_num\": 43,\n",
      "        \"compromised-scada_num\": 57\n",
      "    },\n",
      "    \"attack_logs_num\": {\n",
      "        \"total_num\": 0,\n",
      "        \"external_num\": [],\n",
      "        \"compromised-ied_num\": 0,\n",
      "        \"compromised-scada_num\": 0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset_directory = \"./dataset\" # change this to the folder contain benign and attack subdirs\n",
    "modbus = ModbusDataset(dataset_directory,\"ready\")\n",
    "modbus.summary_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "510ea189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# AutoEncoder (AE)\n",
    "class AE(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder: (89-64-32)\n",
    "    Decoder: (32-64-89)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(89, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 89),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74fb743d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[[1,2,3]]])\n",
    "len(list(a.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "310d2f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ied1b comp ied attack ->\n",
      " test:  4 ['./dataset/ModbusDataset/attack/compromised-scada/ied1b/ied1b-network-captures/ready/vethc76bd3f-6-labeled.csv', './dataset/ModbusDataset/attack/compromised-scada/ied1b/ied1b-network-captures/ready/vethc76bd3f-3-labeled.csv', './dataset/ModbusDataset/attack/compromised-scada/ied1b/ied1b-network-captures/ready/vethc76bd3f-4-labeled.csv', './dataset/ModbusDataset/attack/compromised-scada/ied1b/ied1b-network-captures/ready/vethc76bd3f-1-labeled.csv']\n",
      "network-wide number of csv files ->\n",
      " train : 15 ['./dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-20-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-22-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-25-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-30-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-17-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-31-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-19-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-26-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-29-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-23-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-28-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-15-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-24-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-21-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-16-labeled.csv'] \n",
      " valid: 4 ['./dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-32-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-18-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-27-labeled.csv', './dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-14-labeled.csv']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "csv_files=[col for col in modbus.dataset[\"benign_dataset_dir\"] if col.find(\"network-wide\")!=-1]\n",
    "sys_rand = SystemRandom()\n",
    "sys_rand.shuffle(csv_files)\n",
    "train_files,val_files = train_test_split(csv_files,test_size=0.2,shuffle=True)\n",
    "test_files=[col for col in modbus.dataset[\"attack_dataset_dir\"][\"compromised-scada\"] if col.find(\"ied1b\")!=-1][0:4]\n",
    "print(\"ied1b comp ied attack ->\\n test: \",len(test_files),test_files)\n",
    "print(\"network-wide number of csv files ->\\n train :\",len(train_files),train_files,\"\\n valid:\",len(val_files),val_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4698b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded scalers for 'network-wide'\n"
     ]
    }
   ],
   "source": [
    "loaded_scalers = load_scalers(\"fitted_scalers\")\n",
    "AE_train_dataset=ModbusFlowStream( \n",
    "    shuffle=True,chunk_size=1,batch_size=64,csv_files=train_files,scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=1\n",
    ")\n",
    "AE_train_dataloader=DataLoader(AE_train_dataset,batch_size=1,shuffle=False)\n",
    "AE_val_dataloader=DataLoader(ModbusFlowStream( \n",
    "    shuffle=False,chunk_size=1,batch_size=64,csv_files=val_files,scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=1\n",
    "),batch_size=1,shuffle=False)\n",
    "AE_test_dataloader=DataLoader(ModbusFlowStream(shuffle=False,chunk_size=1,batch_size=64,csv_files=test_files,scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=1),batch_size=1,shuffle=False)\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "AE_model = AE().to(device)\n",
    "lr = 0.01\n",
    "wd= 1e-4\n",
    "shuffle_files =True\n",
    "AE_optimizer = optim.Adam(AE_model.parameters(), lr=lr, weight_decay=wd)\n",
    "criterion = nn.MSELoss(reduction='sum').to(device)\n",
    "eval_criterion = nn.MSELoss(reduction='none').to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2d2e0ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m recon \u001b[38;5;241m=\u001b[39m AE_model(sequences)\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(recon, sequences) \u001b[38;5;241m/\u001b[39m sequences\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m AE_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/labeling/Project1404/FLBased-ICS-NIDS/vnv/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/labeling/Project1404/FLBased-ICS-NIDS/vnv/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/labeling/Project1404/FLBased-ICS-NIDS/vnv/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(10):\n",
    "    time_1 = time.time()\n",
    "    train_loss = 0\n",
    "    AE_model.train()\n",
    "    if shuffle_files:\n",
    "        sys_rand = SystemRandom()\n",
    "        sys_rand.shuffle(AE_train_dataset.file_order_indices)\n",
    "    for sequences, _ in AE_train_dataloader:\n",
    "        sequences=sequences.squeeze().to(device)\n",
    "        AE_optimizer.zero_grad()\n",
    "        recon = AE_model(sequences)\n",
    "        loss = criterion(recon, sequences) / sequences.size(0)\n",
    "        loss.backward()\n",
    "        AE_optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(f\"time {(time.time()-time_1):.4f}\",f\"Epoch {epoch}\", f\"Train Loss: {train_loss / len(AE_train_dataloader):.4f}\")\n",
    "    # Evaluate part\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        AE_model.eval() \n",
    "        all_val_losses = []\n",
    "        all_val_labels = []\n",
    "        print(f\"\\n--- Running Evaluation for Epoch {epoch+1} ---\")\n",
    "        with torch.no_grad():\n",
    "             for sequences, labels in AE_val_dataloader:\n",
    "                sequences = sequences.squeeze().to(device)\n",
    "                recon = AE_model(sequences)\n",
    "                val_loss = eval_criterion(recon, sequences)\n",
    "                if val_loss.dim() > 1:\n",
    "                    intrusion_scores = val_loss.mean(dim=1)\n",
    "                else:\n",
    "                    intrusion_scores = val_loss.unsqueeze(dim=0).mean(dim=1)\n",
    "                intrusion_scores = val_loss.mean(dim=1)\n",
    "                all_val_losses.extend(intrusion_scores.cpu().numpy())\n",
    "                all_val_labels.extend(labels.flatten().cpu().numpy())            \n",
    "        threshold = compute_threshold(all_val_losses)\n",
    "        print(f\"Computed Threshold: {threshold:.4f}\")\n",
    "        all_val_losses = np.array(all_val_losses).squeeze()  \n",
    "        all_val_labels = np.array(all_val_labels).squeeze()  \n",
    "        # If intrusion score > threshold, predict 1 (intrusion), else 0 (benign)\n",
    "        # For FDR, get True Positives (TP) and False Positives (FP)\n",
    "        predictions = (all_val_losses > threshold).astype(int)\n",
    "        accuracy = accuracy_score(all_val_labels, predictions)\n",
    "\n",
    "        print(len(predictions),np.sum(predictions==0),np.sum(predictions))\n",
    "        print(f\"Validation: Accuracy: {accuracy:.4f}  \")\n",
    "        threshold=0.0023\n",
    "        AE_model.eval() \n",
    "        all_test_losses = []\n",
    "        all_test_labels = []\n",
    "        with torch.no_grad():\n",
    "             for sequences, labels in AE_test_dataloader:\n",
    "                sequences = sequences.squeeze().to(device)\n",
    "                recon = AE_model(sequences)\n",
    "                test_loss = eval_criterion(recon, sequences)\n",
    "                if test_loss.dim() > 1:\n",
    "                    intrusion_scores = test_loss.mean(dim=1)\n",
    "                else:\n",
    "                    intrusion_scores = test_loss.unsqueeze(dim=0).mean(dim=1)\n",
    "\n",
    "                all_test_losses.extend(intrusion_scores.cpu().numpy())\n",
    "                all_test_labels.extend(labels.flatten().cpu().numpy())            \n",
    "        all_test_losses = np.array(all_test_losses)\n",
    "        all_test_labels = np.array(all_test_labels) \n",
    "        predictions = (all_test_losses > threshold).astype(int)\n",
    "        binary_test_labels = (all_test_labels != 0).astype(int)\n",
    "        accuracy = accuracy_score(binary_test_labels, predictions)\n",
    "        f1 = f1_score(binary_test_labels, predictions, zero_division=0)\n",
    "        recall = recall_score(binary_test_labels, predictions,zero_division=0)\n",
    "        tn, fp, fn, tp = confusion_matrix(binary_test_labels, predictions, labels=[0, 1]).ravel()\n",
    "        # FDR = FP / (FP + TP) \n",
    "        if (fp + tp) == 0:\n",
    "            fdr = 0.0 \n",
    "        else:\n",
    "            fdr = fp / (fp + tp)\n",
    "        print(len(predictions),np.sum(predictions==0),np.sum(predictions))\n",
    "        print(f\"Test : Accuracy: {accuracy:.4f} Recall : {recall:.4f} FDR: {fdr:.4f}  F1-score: {f1:.4f}  \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Variational AutoEncoder (VAE)\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder: (89-64-64-32 for mu and log_var)\n",
    "    Decoder: (32-64-64-89)\n",
    "    return x_recon, mu, logvar\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(89, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64, 32)\n",
    "        self.fc_logvar = nn.Linear(64, 32)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 89),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "\n",
    "\n",
    "# #Example of iterating\n",
    "# GRU_dataset=ModbusFlowStream(\n",
    "#     shuffle=False,chunk_size=1,batch_size=64,csv_files=modbus.dataset[\"benign_dataset_dir\"][0:2],scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=30\n",
    "# )\n",
    "\n",
    "# GRU_dataloader=DataLoader(GRU_dataset,batch_size=1,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6081d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 20.744476318359375 Epoch 0, Train Loss: 143.65623077564993\n",
      "time 19.662742614746094 Epoch 1, Train Loss: 110.98179194946673\n",
      "time 19.732792854309082 Epoch 2, Train Loss: 94.82658667383795\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "wd= 1e-4\n",
    "\n",
    "VAE_model = VAE().to(device=device)\n",
    "VAE_optimizer = optim.Adam(VAE_model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "AE_dataset=ModbusFlowStream( \n",
    "    shuffle=True,chunk_size=1,batch_size=64,csv_files=modbus.dataset[\"benign_dataset_dir\"][0:2],scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=1\n",
    ")\n",
    "AE_dataloader=DataLoader(AE_dataset,batch_size=1,shuffle=False)\n",
    "\n",
    "for epoch in range(3):\n",
    "    time_1 = time.time()\n",
    "    train_loss = 0\n",
    "    AE_model.train()\n",
    "    if shuffle_files:\n",
    "        sys_rand = SystemRandom()\n",
    "        sys_rand.shuffle(AE_dataset.file_order_indices)\n",
    "    for sequences, _ in AE_dataloader:\n",
    "        sequences = sequences.squeeze().to(device)\n",
    "        VAE_optimizer.zero_grad()\n",
    "        recon, mu, logvar = VAE_model(sequences)\n",
    "        loss = vae_loss_function(recon, sequences, mu, logvar)\n",
    "        loss.backward()\n",
    "        VAE_optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(\"time\",time.time()-time_1,f\"Epoch {epoch}, Train Loss: {train_loss / len(AE_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48a5bd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AAE_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Encoder(Generator):(89-16-4-2)\n",
    "        \"\"\"\n",
    "        super(AAE_Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(89, 16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(16, 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(4, 2))\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "class AAE_Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AAE_Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(4, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 89),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "class AAE_Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AAE_Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(4, 2), # Output for binary classification (real/fake)\n",
    "            nn.Sigmoid()\n",
    "        )    \n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ea5785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "aae_encoder = AAE_Encoder()\n",
    "aae_decoder = AAE_Decoder()\n",
    "aae_discriminator = AAE_Discriminator()\n",
    "aae_encoder.to(device)\n",
    "aae_decoder.to(device)\n",
    "aae_discriminator.to(device)\n",
    "optimizer_G = optim.Adam(list(aae_encoder.parameters()) + list(aae_decoder.parameters()), lr=1e-4)\n",
    "optimizer_D = optim.Adam(aae_discriminator.parameters(), lr=1e-4)\n",
    "adversarial_loss = nn.BCELoss(reduction=\"sum\")\n",
    "reconstruction_loss = nn.MSELoss(reduction=\"sum\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f371a05",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aae_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43maae_encoder\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m     aae_decoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m     aae_discriminator\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aae_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs=5\n",
    "for epoch in range(num_epochs):\n",
    "    aae_encoder.train()\n",
    "    aae_decoder.train()\n",
    "    aae_discriminator.train()\n",
    "    if shuffle_files:\n",
    "        sys_rand = SystemRandom()\n",
    "        sys_rand.shuffle(AE_dataset.file_order_indices)\n",
    "    for sequences,_ in AE_dataloader:\n",
    "        sequences=sequences.squeeze().to(device)\n",
    "        # 1) reconstruction + generator loss\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_z = aae_encoder(sequences)\n",
    "        decoded_seq = aae_decoder(fake_z)\n",
    "        G_loss = 0.001*adversarial_loss(aae_discriminator(fake_z),  torch.ones(sequences.size(0), 2,device=device)) + 0.999*reconstruction_loss(decoded_seq, sequences)\n",
    "        G_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        # 2) discriminator loss\n",
    "        optimizer_D.zero_grad()\n",
    "        real_loss = adversarial_loss(aae_discriminator(torch.randn(sequences.size(0), 2, device=device)),  torch.ones(sequences.size(0), 2,device=device))\n",
    "        fake_loss = adversarial_loss(aae_discriminator(fake_z.detach()),  torch.zeros(sequences.size(0), 2,device=device))\n",
    "        D_loss = 0.5*(real_loss + fake_loss)\n",
    "        D_loss.backward()\n",
    "        optimizer_D.step()\n",
    "    # print loss\n",
    "    print(\n",
    "            \"[Epoch %d/%d] [G loss: %f] [D loss: %f]\"\n",
    "            % (epoch, num_epochs, G_loss.item(), D_loss.item())\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3698c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU-VAE\n",
    "class GRUVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Recurrent Unit : num_layers=2, hidden_size=256, dropout=0.01,window size (seq_len)= 40\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=89, hidden_dim=256, latent_dim=32, num_layers=2, dropout=0.01):\n",
    "        super(GRUVAE, self).__init__()\n",
    "        self.encoder_gru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_z_to_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_gru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, input_dim=89]\n",
    "        _, hidden = self.encoder_gru(x) \n",
    "        h = hidden[-1]  # [batch_size, hidden_dim]\n",
    "        mu = self.fc_mu(h)  \n",
    "        logvar = self.fc_logvar(h)  \n",
    "        z = self.reparameterize(mu, logvar)  # [batch_size, latent_dim]\n",
    "        # repeat and feed latent z as input trick\n",
    "        h0 = self.fc_z_to_hidden(z).unsqueeze(0).repeat(self.encoder_gru.num_layers, 1, 1)  # [num_layers, batch_size, hidden_dim]\n",
    "        # Initialize decoder input with zeros \n",
    "        decoder_input = torch.zeros_like(x)\n",
    "        output, _ = self.decoder_gru(decoder_input, h0)  # [batch_size, seq_len, hidden_dim]\n",
    "        x_recon = self.fc_out(output)  # [batch_size, seq_len, input_dim]\n",
    "        return x_recon, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799e51d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded scalers for 'network-wide'\n"
     ]
    }
   ],
   "source": [
    "loaded_scalers = load_scalers(\"fitted_scalers\")\n",
    "RNN_dataset=ModbusFlowStream( \n",
    "    shuffle=False,chunk_size=1,batch_size=64,csv_files=modbus.dataset[\"benign_dataset_dir\"][0:2],\n",
    "    scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=30\n",
    ")\n",
    "RNN_dataloadder=DataLoader(RNN_dataset,batch_size=1,shuffle=False)\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "GRU_VAE_model = GRUVAE().to(device)\n",
    "lr = 0.01\n",
    "wd= 1e-4\n",
    "shuffle_files =True\n",
    "GRU_VAE_optimizer = optim.Adam(GRU_VAE_model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97bb352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 103.30588173866272 Epoch 0, Train Loss: 127700652.60516566\n",
      "time 95.60702395439148 Epoch 1, Train Loss: 70232281.69612122\n",
      "time 94.19640827178955 Epoch 2, Train Loss: 3457994.1813964844\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    time_1 = time.time()\n",
    "    train_loss = 0\n",
    "    GRU_VAE_model.train()\n",
    "    if shuffle_files:\n",
    "        sys_rand = SystemRandom()\n",
    "        sys_rand.shuffle(RNN_dataset.file_order_indices)\n",
    "    for sequences, _ in RNN_dataloadder:\n",
    "        sequences = sequences.squeeze().to(device)\n",
    "        GRU_VAE_optimizer.zero_grad()\n",
    "        recon, mu, logvar = GRU_VAE_model(sequences)\n",
    "        loss = vae_loss_function(recon, sequences, mu, logvar)/sequences.size(0)\n",
    "        loss.backward()\n",
    "        GRU_VAE_optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(\"time\",time.time()-time_1,f\"Epoch {epoch}, Train Loss: {train_loss/len(RNN_dataloadder)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
