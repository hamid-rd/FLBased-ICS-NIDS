{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32826f1-1369-4eb5-a21b-0e6e548cded3",
   "metadata": {},
   "source": [
    "### Download and make the dataset ready in Kaggle \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c2ad5c-f43a-4b40-a647-ecc58c71ff16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T16:17:38.693282Z",
     "iopub.status.busy": "2025-07-13T16:17:38.692671Z",
     "iopub.status.idle": "2025-07-13T16:20:31.990912Z",
     "shell.execute_reply": "2025-07-13T16:20:31.990108Z",
     "shell.execute_reply.started": "2025-07-13T16:17:38.693248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ## uncomment if The zip file of the dataset isn't downloaded,extraced \n",
    "# !pip install gdown\n",
    "# Copy the link. The file ID is the long string of characters between d/ and /view.\n",
    "#For example, in the URL https://drive.google.com/file/d/1aBcDeFgHiJkLmNoPqRsTuVwXyZ/view?usp=sharing, \n",
    "#the file ID is 1aBcDeFgHiJkLmNoPqRsTuVwXyZ\n",
    "# !mkdir /kaggle/tmp\n",
    "# !gdown  1pzXpA5Cz0DJmjRsLxlqRNnJq-kOUvojb -O /kaggle/tmp/Labeled_CICMODBUS2023.zip\n",
    "# !unzip /kaggle/tmp/Labeled_CICMODBUS2023.zip -d /kaggle/working/\n",
    "\n",
    "# # ## uncomment if the python modules (modbus.py,utils.py ,...) not cloned  and added to the path \n",
    "\n",
    "# !git clone https://github.com/hamid-rd/FLBased-ICS-NIDS.git\n",
    "# import sys\n",
    "# # Add the repository folder to the Python path\n",
    "# repo_path = '/kaggle/working/FLBased-ICS-NIDS'\n",
    "# repo_input_path = '/kaggle/input/training/FLBased-ICS-NIDS'\n",
    "# dataset_path = '/kaggle/input/training/'\n",
    "\n",
    "# for path in {repo_path,repo_input_path,dataset_path}:\n",
    "#     if path not in sys.path:\n",
    "#         sys.path.append(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601bb537-782e-4266-b619-48cdad4fe6a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T16:25:34.078617Z",
     "iopub.status.busy": "2025-07-13T16:25:34.077721Z",
     "iopub.status.idle": "2025-07-13T16:25:34.430103Z",
     "shell.execute_reply": "2025-07-13T16:25:34.429493Z",
     "shell.execute_reply.started": "2025-07-13T16:25:34.078584Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The CIC Modbus Dataset contains network (pcap) captures and attack logs from a simulated substation network.\n",
      "                The dataset is categorized into two groups: an attack dataset and a benign dataset\n",
      "                The attack dataset includes network traffic captures that simulate various types of Modbus protocol attacks in a substation environment.\n",
      "                The attacks are reconnaissance, query flooding, loading payloads, delay response, modify length parameters, false data injection, stacking Modbus frames, brute force write and baseline replay.\n",
      "                These attacks are based of some techniques in the MITRE ICS ATT&CK framework.\n",
      "                On the other hand, the benign dataset consists of normal network traffic captures representing legitimate Modbus communication within the substation network.\n",
      "                The purpose of this dataset is to facilitate research, analysis, and development of intrusion detection systems, anomaly detection algorithms and other security mechanisms for substation networks using the Modbus protocol.\n",
      "                https://www.unb.ca/cic/datasets/modbus-2023.html\n",
      "                In my custom PyTorch Dataset class,\n",
      "                I utilize the Enhanced CICflowMeter and the Attack logs correlation to extract and label sequential data flows,\n",
      "                preparing them for batch processing with the DataLoader, which is crucial for AI model training.\n",
      "                https://github.com/hamid-rd/FLBased-ICS-NIDS/tree/main\n",
      "\n",
      "                \n",
      "csv files  in the dataset directory founded with the filter:  ready\n",
      "{\n",
      "    \"total_dataset_num\": 170,\n",
      "    \"benign_dataset_num\": 62,\n",
      "    \"attack_dataset_num\": {\n",
      "        \"total_num\": 108,\n",
      "        \"external_num\": 8,\n",
      "        \"compromised-ied_num\": 43,\n",
      "        \"compromised-scada_num\": 57\n",
      "    },\n",
      "    \"attack_logs_num\": {\n",
      "        \"total_num\": 0,\n",
      "        \"external_num\": [],\n",
      "        \"compromised-ied_num\": 0,\n",
      "        \"compromised-scada_num\": 0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# To test if every thing is okay (modbus.py class and correct number of founded csv files )\n",
    "from modbus import ModbusDataset,ModbusFlowStream\n",
    "\n",
    "# dataset_directory = \"/kaggle/working/ModbusDataset\" \n",
    "# dataset_directory = \"/kaggle/input/training/ModbusDataset\" \n",
    "dataset_directory = \"dataset\" \n",
    "\n",
    "modbus = ModbusDataset(dataset_directory,\"ready\")\n",
    "modbus.summary_print()\n",
    "\n",
    "# Don't forget to save version in kaggle (to save outputs written on the disk (/kaggle/working/))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d238262",
   "metadata": {},
   "source": [
    "### Unsupervised GRU-VAE training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c01e06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T16:25:38.158958Z",
     "iopub.status.busy": "2025-07-13T16:25:38.158509Z",
     "iopub.status.idle": "2025-07-13T16:25:38.167563Z",
     "shell.execute_reply": "2025-07-13T16:25:38.166807Z",
     "shell.execute_reply.started": "2025-07-13T16:25:38.158938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np # For standard deviation calculation\n",
    "from modbus import ModbusDataset,ModbusFlowStream\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix,recall_score\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from utils import load_scalers\n",
    "from random import SystemRandom\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "def compute_threshold(mse_values,k=1):\n",
    "\n",
    "    \"\"\"\n",
    "    K-SIGMA\n",
    "    Computes the anomaly detection threshold (for marking sample as Intrusion if the IS was greater )\n",
    "    based on the mean and standard deviation of Mean Squared Error (MSE) values.\n",
    "    Formula: thr = mean(MSE) + std(MSE)\n",
    "    Args:\n",
    "    mse_values (torch.Tensor or list/np.array): A tensor or list of MSE values\n",
    "\n",
    "                            obtained from the validation set.\n",
    "    Returns:\n",
    "    float: The calculated threshold.\n",
    "    float: The calculated std.\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(mse_values, torch.Tensor):\n",
    "        mse_values = torch.tensor(mse_values, dtype=torch.float32)\n",
    "    if mse_values.numel() == 0:\n",
    "        return 0.0\n",
    "    mean_mse = torch.mean(mse_values)\n",
    "    std_mse = torch.std(mse_values)\n",
    "    print(\"-----------mse_loss mean : \",f\"{mean_mse.item():.4f}\",\"std:\",f\"{std_mse.item():.4f}\")\n",
    "    threshold = mean_mse + k*std_mse\n",
    "    return threshold.item(),std_mse.item()\n",
    "\n",
    "def vae_loss_function(recon_x, x, mu, logvar,beta =1):\n",
    "    \"\"\"\n",
    "    VAE loss function.\n",
    "    \"\"\"\n",
    "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return (BCE + beta*KLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9478520",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T16:25:42.807302Z",
     "iopub.status.busy": "2025-07-13T16:25:42.807033Z",
     "iopub.status.idle": "2025-07-13T16:25:42.941019Z",
     "shell.execute_reply": "2025-07-13T16:25:42.940221Z",
     "shell.execute_reply.started": "2025-07-13T16:25:42.807283Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The CIC Modbus Dataset contains network (pcap) captures and attack logs from a simulated substation network.\n",
      "                The dataset is categorized into two groups: an attack dataset and a benign dataset\n",
      "                The attack dataset includes network traffic captures that simulate various types of Modbus protocol attacks in a substation environment.\n",
      "                The attacks are reconnaissance, query flooding, loading payloads, delay response, modify length parameters, false data injection, stacking Modbus frames, brute force write and baseline replay.\n",
      "                These attacks are based of some techniques in the MITRE ICS ATT&CK framework.\n",
      "                On the other hand, the benign dataset consists of normal network traffic captures representing legitimate Modbus communication within the substation network.\n",
      "                The purpose of this dataset is to facilitate research, analysis, and development of intrusion detection systems, anomaly detection algorithms and other security mechanisms for substation networks using the Modbus protocol.\n",
      "                https://www.unb.ca/cic/datasets/modbus-2023.html\n",
      "                In my custom PyTorch Dataset class,\n",
      "                I utilize the Enhanced CICflowMeter and the Attack logs correlation to extract and label sequential data flows,\n",
      "                preparing them for batch processing with the DataLoader, which is crucial for AI model training.\n",
      "                https://github.com/hamid-rd/FLBased-ICS-NIDS/tree/main\n",
      "\n",
      "                \n",
      "csv files  in the dataset directory founded with the filter:  ready\n",
      "{\n",
      "    \"total_dataset_num\": 170,\n",
      "    \"benign_dataset_num\": 62,\n",
      "    \"attack_dataset_num\": {\n",
      "        \"total_num\": 108,\n",
      "        \"external_num\": 8,\n",
      "        \"compromised-ied_num\": 43,\n",
      "        \"compromised-scada_num\": 57\n",
      "    },\n",
      "    \"attack_logs_num\": {\n",
      "        \"total_num\": 0,\n",
      "        \"external_num\": [],\n",
      "        \"compromised-ied_num\": 0,\n",
      "        \"compromised-scada_num\": 0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# dataset_directory = \"/kaggle/input/training/ModbusDataset\" # change this to the folder contain benign and attack subdirs\n",
    "dataset_directory = \"dataset\" \n",
    "modbus = ModbusDataset(dataset_directory,\"ready\")\n",
    "modbus.summary_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "510ea189",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T16:14:44.542531Z",
     "iopub.status.busy": "2025-07-13T16:14:44.541882Z",
     "iopub.status.idle": "2025-07-13T16:14:44.550154Z",
     "shell.execute_reply": "2025-07-13T16:14:44.549125Z",
     "shell.execute_reply.started": "2025-07-13T16:14:44.542501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _init_weights( module):\n",
    "    ## for one layer apply Xavier Initialization\n",
    "    if isinstance(module, nn.Linear):\n",
    "        init.xavier_normal_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            init.zeros_(module.bias)\n",
    "    return module\n",
    "# AutoEncoder (AE)\n",
    "class AE(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder: (89-64-32)\n",
    "    Decoder: (32-64-89)\n",
    "    \"\"\"\n",
    "    def __init__(self,input_dim=89):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2d2e0ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:02:58.408647Z",
     "iopub.status.busy": "2025-07-13T08:02:58.407931Z",
     "iopub.status.idle": "2025-07-13T08:02:58.430831Z",
     "shell.execute_reply": "2025-07-13T08:02:58.430062Z",
     "shell.execute_reply.started": "2025-07-13T08:02:58.408625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def train_eval(model,train_dataloader,val_dataloader,test_dataloader,learning_rates= [5e-6,1e-7,5e-5,1e-5,1e-6],\n",
    "               weight_decays=[1e-5,1e-4,1e-7],shuffle_files=True,num_epochs=20,eval_epoch=4,criterion_method=\"mse\"):\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model=model.to(device)\n",
    "    if criterion_method==\"bce\":\n",
    "        criterion = nn.BCELoss(reduction='sum').to(device)\n",
    "        eval_criterion = nn.BCELoss(reduction='none').to(device)\n",
    "    else: #mse\n",
    "        criterion = nn.MSELoss(reduction='sum').to(device)\n",
    "        eval_criterion = nn.MSELoss(reduction='none').to(device)\n",
    "    best_f1=0 #to save best version of the model during test\n",
    "    best_recall=0 #to save best version of the model during test\n",
    "\n",
    "    for lr, wd in itertools.product(learning_rates, weight_decays):\n",
    "        if model._get_name()==\"AdversarialAutoencoder\":\n",
    "            adversarial_criterion= nn.BCELoss(reduction=\"sum\")\n",
    "            optimizer_D = optim.SGD(model.discriminator.parameters(), lr=lr, weight_decay=wd)\n",
    "            optimizer_G =  optim.SGD(list(model.encoder.parameters()) + list(model.decoder.parameters()), lr=lr, weight_decay=wd)\n",
    "        else:\n",
    "            AE_optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "            ### new code\n",
    "            # AE_optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        print(f\"\\n==================  lr={lr}, wd={wd} ==================\")\n",
    "        model.apply(_init_weights)\n",
    "        for epoch in range(num_epochs):\n",
    "            time_1 = time.time()\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            ## for AAE\n",
    "            Discriminator_loss = 0\n",
    "            if shuffle_files:\n",
    "                sys_rand = SystemRandom()\n",
    "                sys_rand.shuffle(train_dataloader.dataset.csv_files)\n",
    "            for sequences, labels in train_dataloader:\n",
    "                sequences=sequences.squeeze().to(device)\n",
    "                if labels.sum()!=0:\n",
    "                    continue\n",
    "                if model._get_name()==\"AdversarialAutoencoder\":\n",
    "                    target_ones= torch.ones(sequences.size(0), 1,device=device,dtype=torch.float)\n",
    "                    target_zeros= torch.zeros(sequences.size(0), 1,device=device,dtype=torch.float)\n",
    "                    random_latent = torch.randn(sequences.size(0), 2, device=device)\n",
    "                    optimizer_G.zero_grad()\n",
    "                    fake_z,decoded_seq = model(sequences)\n",
    "                    G_loss = 0.001*adversarial_criterion(model.discriminator(fake_z),target_ones ) + 0.999*criterion(decoded_seq, sequences)\n",
    "                    G_loss.backward()\n",
    "                    optimizer_G.step()\n",
    "                    # 2) discriminator loss\n",
    "                    optimizer_D.zero_grad()\n",
    "                    real_loss = adversarial_criterion(model.discriminator(random_latent), target_ones)\n",
    "                    fake_loss = adversarial_criterion(model.discriminator(fake_z.detach()),  target_zeros)\n",
    "                    D_loss = 0.5*(real_loss + fake_loss)\n",
    "                    D_loss.backward()\n",
    "                    optimizer_D.step()\n",
    "                    train_loss+=G_loss.item()\n",
    "                    Discriminator_loss+=D_loss.item()   \n",
    "                else:\n",
    "                    AE_optimizer.zero_grad()\n",
    "                    if model._get_name()==\"AE\":\n",
    "                        recon = model(sequences)\n",
    "                        loss = criterion(recon, sequences) / sequences.size(0)\n",
    "                    elif model._get_name()==\"VAE\" or model._get_name()==\"GRUVAE\":\n",
    "                        recon, mu, logvar = model(sequences)\n",
    "                        loss = vae_loss_function(recon, sequences, mu, logvar) /sequences.size(0)\n",
    "                    loss.backward()\n",
    "                    AE_optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "            print(f\"Train : time {(time.time()-time_1):.2f} s\",\n",
    "            f\"Epoch {epoch+1}\")\n",
    "            if model._get_name()==\"AdversarialAutoencoder\":\n",
    "                print(f\"Generator Loss: {train_loss / len(train_dataloader):.4f}\",\n",
    "                    f\"Discriminator Loss: {Discriminator_loss / len(train_dataloader):.4f}\")\n",
    "            else:\n",
    "                print(f\"Train Loss: {train_loss / len(train_dataloader):.4f}\")\n",
    "            # Evaluate part\n",
    "            if (epoch + 1) % eval_epoch == 0:\n",
    "                model.eval() \n",
    "                all_val_losses = []\n",
    "                all_val_labels = []\n",
    "                print(f\"--- Running Evaluation for Epoch {epoch+1} lr ={lr} wd {wd} ---\")\n",
    "                with torch.no_grad():\n",
    "                    for sequences, labels in val_dataloader:\n",
    "                        sequences = sequences.squeeze().to(device) \n",
    "                        if labels.sum()!=0:\n",
    "                            continue\n",
    "                        if criterion_method==\"bce\":\n",
    "                            ## may test features be greater than 1 after scaling \n",
    "                            sequences=torch.clamp(sequences, min=0.0, max=1.0)      \n",
    "                        if model._get_name()==\"AE\":\n",
    "                            recon = model(sequences)\n",
    "                        elif model._get_name()==\"VAE\" or model._get_name()==\"GRUVAE\" :\n",
    "                            recon, _, _ = model(sequences)\n",
    "                        elif model._get_name()==\"AdversarialAutoencoder\":\n",
    "                            _,recon= model(sequences)\n",
    "\n",
    "                        val_loss = eval_criterion(recon, sequences)\n",
    "                        if val_loss.dim() > 1:\n",
    "                            val_loss = val_loss\n",
    "                        else:\n",
    "                            val_loss = val_loss.unsqueeze(dim=0)\n",
    "                            labels = labels.unsqueeze(dim=0)\n",
    "                        val_loss = val_loss.sum(dim=1)\n",
    "                        all_val_losses.extend(val_loss.cpu().numpy())\n",
    "                        all_val_labels.extend(labels.flatten().cpu().numpy())            \n",
    "                threshold_1,std_mse = compute_threshold(all_val_losses,k=0)\n",
    "\n",
    "                all_val_losses = np.array(all_val_losses).squeeze()  \n",
    "                all_val_labels = np.array(all_val_labels).squeeze()  \n",
    "                # If intrusion score > threshold, predict 1 (intrusion), else 0 (benign)\n",
    "                # For FDR, get True Positives (TP) and False Positives (FP)\n",
    "                \n",
    "                predictions = (all_val_losses > threshold_1).astype(int)\n",
    "                accuracy = accuracy_score(all_val_labels, predictions)\n",
    "                print(f\"Val: Accuracy: {accuracy:.4f}  \")\n",
    "                model.eval() \n",
    "                all_test_losses = []\n",
    "                all_test_labels = []\n",
    "                with torch.no_grad():\n",
    "                    for sequences, labels in test_dataloader:\n",
    "                        sequences = sequences.squeeze().to(device)\n",
    "                        labels = labels.squeeze().to(device)\n",
    "                        if criterion_method==\"bce\":\n",
    "                            ## may test features be greater than 1 after scaling \n",
    "                            sequences=torch.clamp(sequences, min=0.0, max=1.0)\n",
    "                        if model._get_name()==\"AE\":\n",
    "                            recon = model(sequences)\n",
    "                        elif model._get_name()==\"VAE\"  or model._get_name()==\"GRUVAE\":\n",
    "                            recon, mu, logvar = model(sequences)\n",
    "                        elif model._get_name()==\"AdversarialAutoencoder\":\n",
    "                            _,recon= model(sequences)\n",
    "\n",
    "                        intrusion_scores = eval_criterion(recon, sequences)\n",
    "                        if intrusion_scores.dim() > 1:\n",
    "                            intrusion_scores = intrusion_scores\n",
    "                        else:\n",
    "                            intrusion_scores = intrusion_scores.unsqueeze(dim=0)\n",
    "                            labels = labels.unsqueeze(dim=0)\n",
    "                        intrusion_scores = intrusion_scores.sum(dim=1)\n",
    "                        all_test_losses.extend(intrusion_scores.cpu().numpy())\n",
    "                        all_test_labels.extend(labels.cpu().numpy())\n",
    "                        ### remove labels =1 (don't consider Brute Force attack beacuse of conflicted labeling with normal data)\n",
    "                        # mask = labels != 1\n",
    "\n",
    "                        # filtered_scores = intrusion_scores[mask]\n",
    "                        # filtered_labels = labels[mask]\n",
    "                \n",
    "                        # # Move to CPU and convert to numpy\n",
    "                        # all_test_losses.extend(filtered_scores.cpu().numpy())\n",
    "                        # all_test_labels.extend(filtered_labels.cpu().numpy())\n",
    "\n",
    "\n",
    "                all_test_losses = np.array(all_test_losses)\n",
    "                all_test_labels = np.array(all_test_labels)\n",
    "                # for threshold in {threshold_1,threshold_2,threshold_3,threshold_10,threshold_100}:\n",
    "                temp_best_recall =best_recall\n",
    "                temp_best_f1 =best_f1\n",
    "\n",
    "                for k in {1,3}:\n",
    "                    threshold=threshold_1+k*std_mse\n",
    "                    print(f\" K: {k} K-SIGMA Threshold : ---thr {threshold:.4}\")\n",
    "                    predictions = (all_test_losses > threshold).astype(int)\n",
    "                    binary_test_labels = (all_test_labels != 0).astype(int)\n",
    "                    # --- Start of new code ---\n",
    "\n",
    "                    # Find the indices where the prediction was incorrect\n",
    "                    misclassified_indices = np.where(binary_test_labels != predictions)[0]\n",
    "\n",
    "                    # Get the original labels for those misclassified instances\n",
    "                    misclassified_original_labels = all_test_labels[misclassified_indices]\n",
    "\n",
    "                    # To get a summary count of which labels were misclassified\n",
    "                    print(Counter(predictions),Counter(binary_test_labels))\n",
    "                    print(f\"Counts of  labels: {dict(sorted(Counter(all_test_labels).items()))}\")\n",
    "                    print(f\"Counts of misclassified original labels: {dict(sorted(Counter(misclassified_original_labels).items()))}\")\n",
    "\n",
    "                    accuracy = accuracy_score(binary_test_labels, predictions)\n",
    "                    f1 = f1_score(binary_test_labels, predictions, zero_division=0)\n",
    "                    recall = recall_score(binary_test_labels, predictions,zero_division=0)\n",
    "                    _, fp, _, tp = confusion_matrix(binary_test_labels, predictions, labels=[0, 1]).ravel()\n",
    "                    # FDR = FP / (FP + TP) \n",
    "                    if (fp + tp) == 0:\n",
    "                        fdr = 0.0 \n",
    "                    else:\n",
    "                        fdr = fp / (fp + tp)\n",
    "                    print(f\"Test : Accuracy: {accuracy:.4f} Recall : {recall:.4f} FDR: {fdr:.4f}  F1-score: {f1:.4f}  \")\n",
    "                    !mkdir best_models -p\n",
    "                    if k==3 and f1>best_f1 :\n",
    "                        best_f1=f1\n",
    "                    elif k==1 and recall>best_recall:\n",
    "                        best_recall=recall\n",
    "                if (best_recall>temp_best_recall or best_f1 > temp_best_f1):\n",
    "                    save_path =\"best_models/\"+model._get_name()+\"_f1_\"+f\"{best_f1:.2f}\" +\"_recall_\"+f\"{best_recall:.2f}\" +\"_.pth\"\n",
    "                    torch.save(model.state_dict(),save_path)\n",
    "                    print(\"model\",model._get_name(),\"is saved in\" ,save_path )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfeeeaf",
   "metadata": {},
   "source": [
    "#### Centralized : external scenario -> ied1a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db440aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ied1b comp ied attack ->\n",
      " test:  1 ['dataset/ModbusDataset/attack/external/ied1a/ied1a-network-capture/ready/veth4edc015-0-labeled.csv']\n",
      "----------- Network-wide number of csv files -> \n",
      " ----------- train : 19 ['dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-16-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-28-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-22-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-18-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-30-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-20-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-26-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-32-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-15-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-29-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-23-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-19-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-25-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-27-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-24-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-31-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-17-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-21-labeled.csv', 'dataset/ModbusDataset/benign/network-wide-pcap-capture/network-wide/ready/network-wide-normal-14-labeled.csv'] \n",
      " -------- valid: 8 ['dataset/ModbusDataset/benign/ied1a/ied1a-network-capture/ready/veth4edc015-normal-10-labeled.csv', 'dataset/ModbusDataset/benign/ied1a/ied1a-network-capture/ready/veth4edc015-normal-9-labeled.csv', 'dataset/ModbusDataset/benign/ied1a/ied1a-network-capture/ready/veth4edc015-normal-11-labeled.csv', 'dataset/ModbusDataset/benign/ied1a/ied1a-network-capture/ready/veth4edc015-normal-6-labeled.csv', 'dataset/ModbusDataset/benign/ied1a/ied1a-network-capture/ready/veth4edc015-normal-5-labeled.csv', 'dataset/ModbusDataset/benign/ied1a/ied1a-network-capture/ready/veth4edc015-normal-7-labeled.csv', 'dataset/ModbusDataset/benign/ied1a/ied1a-network-capture/ready/veth4edc015-normal-8-labeled.csv', 'dataset/ModbusDataset/benign/ied1a/ied1a-network-capture/ready/veth4edc015-normal-4-labeled.csv']\n"
     ]
    }
   ],
   "source": [
    "# train_files=[col for col in modbus.dataset[\"benign_dataset_dir\"] if col.find(\"network-wide\")!=-1]\n",
    "train_files=[col for col in modbus.dataset[\"benign_dataset_dir\"] if col.find(\"network-wide\")!=-1][:]\n",
    "val_files = [col for col in modbus.dataset[\"benign_dataset_dir\"] if col.find(\"ied1a\")!=-1][:]\n",
    "test_files=[col for col in modbus.dataset[\"attack_dataset_dir\"][\"external\"] if col.find(\"ied1a\")!=-1]\n",
    "sys_rand = SystemRandom()\n",
    "\n",
    "sys_rand.shuffle(train_files)\n",
    "sys_rand.shuffle(val_files)\n",
    "sys_rand.shuffle(test_files)\n",
    "\n",
    "\n",
    "print(\"ied1b comp ied attack ->\\n test: \",len(test_files),test_files)\n",
    "print(\"----------- Network-wide number of csv files -> \\n ----------- train :\",len(train_files),train_files,\"\\n -------- valid:\",len(val_files),val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2505cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded scalers for 'network-wide'\n"
     ]
    }
   ],
   "source": [
    "### Try The Copy-on-Write (CoW) technique, share the same single copy of the dataset in memory with multiple forked workers from the main process\n",
    "# Ensure to have enough memory for saving large tensors in the ram \n",
    "###### else use chunk_size =1 and read the files iteratively\n",
    "\n",
    "use_cow=True\n",
    "window_size=1\n",
    "loaded_scalers=load_scalers('fitted_scalers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eac4698b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:12:06.828444Z",
     "iopub.status.busy": "2025-07-13T08:12:06.827779Z",
     "iopub.status.idle": "2025-07-13T08:12:07.034947Z",
     "shell.execute_reply": "2025-07-13T08:12:07.034115Z",
     "shell.execute_reply.started": "2025-07-13T08:12:06.828415Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cow Processing datasets...\n",
      "  - Creating 'train' dataset...\n",
      "  - Finished 'train' dataset.\n",
      "  - Creating 'val' dataset...\n",
      "  - Finished 'val' dataset.\n",
      "  - Creating 'test' dataset...\n",
      "  - Finished 'test' dataset.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# This cell Initializes and returns train, validation, and test dataloaders.\n",
    "\n",
    "# This function supports two strategies for data loading:\n",
    "# 1. Copy-on-Write (use_cow=True): Loads the entire dataset into RAM. This is fast\n",
    "#     but memory-intensive. It allows multiple worker processes to share the same\n",
    "#     dataset copy in memory, which is efficient for multiprocessing.\n",
    "# 2. Iterative (use_cow=False): Reads data from files in small chunks. This is\n",
    "#     slower but uses significantly less memory, suitable for very large datasets\n",
    "#     that don't fit in RAM.\n",
    "\n",
    "#     train_files (list): List of file paths for the training dataset.\n",
    "#     val_files (list): List of file paths for the validation dataset.\n",
    "#     test_files (list): List of file paths for the test dataset.\n",
    "#     window_size (int): The size of the sliding window for sequence data.\n",
    "#     use_cow (bool, optional): If True, uses the Copy-on-Write strategy. \n",
    "#                                 Defaults to True.\n",
    "\n",
    "#      return        (train_dataloader, val_dataloader, test_dataloader)\n",
    "\n",
    "if use_cow==True:\n",
    "    large_chunk_size = modbus.dataset[\"metadata\"][\"founded_files_num\"][\"total_dataset_num\"]\n",
    "\n",
    "    dataset_configs = {\n",
    "        \"train\": {\"files\": train_files},\n",
    "        \"val\": {\"files\": val_files},\n",
    "        \"test\": {\"files\": test_files},\n",
    "    }\n",
    "    datasets = {}\n",
    "    ae_datasets = {}\n",
    "\n",
    "    print(\"Cow Processing datasets...\")\n",
    "\n",
    "    for name, config in dataset_configs.items():\n",
    "        print(f\"  - Creating '{name}' dataset...\")\n",
    "        \n",
    "        # 1. Create the primary ModbusFlowStream dataset\n",
    "        datasets[name] = ModbusFlowStream(\n",
    "            shuffle=False,\n",
    "            chunk_size=large_chunk_size,\n",
    "            batch_size=1,\n",
    "            csv_files=config[\"files\"],\n",
    "            scalers=loaded_scalers['network-wide']['min_max_scalers'],\n",
    "            window_size=window_size\n",
    "        )\n",
    "        \n",
    "        # 2. Call __getitem__(0) once to load the entire dataset chunk into memory\n",
    "        datasets[name].__getitem__(0)\n",
    "        \n",
    "        # used for specific AE training/evaluation without re-reading files.\n",
    "        ae_datasets[name] = ModbusFlowStream(\n",
    "            shuffle=False,  # AE data is typically processed in order\n",
    "            chunk_size=large_chunk_size,\n",
    "            batch_size=1,\n",
    "            csv_files=[],  # No CSV files needed as we copy the data directly\n",
    "            scalers=None,   # Data is already scaled from the original dataset\n",
    "            window_size=window_size\n",
    "        )\n",
    "        \n",
    "        # 4. Manually copy the loaded data and properties to the AE dataset\n",
    "\n",
    "        ae_datasets[name].current_chunk_data =  datasets[name].current_chunk_data\n",
    "        ae_datasets[name].current_len_chunk_data =  datasets[name].current_len_chunk_data\n",
    "        ae_datasets[name].current_chunk_labels =  datasets[name].current_chunk_labels\n",
    "        ae_datasets[name].total_batches =  datasets[name].total_batches\n",
    "        \n",
    "        print(f\"  - Finished '{name}' dataset.\")\n",
    "    train_dataloader=DataLoader(ae_datasets['train'],batch_size=64,shuffle=True,num_workers=4,persistent_workers=True,prefetch_factor=2,pin_memory=True)\n",
    "    val_dataloader=DataLoader(ae_datasets['val'],batch_size=64,shuffle=False,num_workers=4,persistent_workers=True,prefetch_factor=2,pin_memory=True)\n",
    "    test_dataloader=DataLoader(ae_datasets['test'],batch_size=64,shuffle=False,num_workers=4,persistent_workers=True,prefetch_factor=2,pin_memory=True)\n",
    "\n",
    "else :\n",
    "    train_dataloader=DataLoader(ModbusFlowStream( \n",
    "        shuffle=True,chunk_size=1,batch_size=64,csv_files=train_files,scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=window_size\n",
    "    ),  batch_size=1,shuffle=False)\n",
    "    val_dataloader=DataLoader(ModbusFlowStream( \n",
    "        shuffle=False,chunk_size=1,batch_size=64,csv_files=val_files,scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=window_size\n",
    "    ),batch_size=1,shuffle=False)\n",
    "    test_dataloader=DataLoader(ModbusFlowStream(shuffle=False,chunk_size=1,batch_size=64,csv_files=test_files,scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=window_size),\n",
    "                               batch_size=1,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69406aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44101 19154 1960\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader),len(val_dataloader),len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82123e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================  lr=5e-05, wd=0.0001 ==================\n",
      "Train : time 163.55 s Epoch 1\n",
      "Train Loss: 0.2204\n",
      "--- Running Evaluation for Epoch 1 lr =5e-05 wd 0.0001 ---\n",
      "-----------mse_loss mean :  0.0031 std: 0.0626\n",
      "Val: Accuracy: 0.9751  \n",
      " K: 1 K-SIGMA Threshold : ---thr 0.06569\n",
      "Counter({1: 65686, 0: 59746}) Counter({0: 89417, 1: 36015})\n",
      "Counts of  labels: {0: 89417, 1: 35978, 2: 1, 3: 1, 4: 1, 5: 2, 6: 31, 7: 1}\n",
      "Counts of misclassified original labels: {0: 29808, 1: 137}\n",
      "Test : Accuracy: 0.7613 Recall : 0.9962 FDR: 0.4538  F1-score: 0.7056  \n",
      " K: 3 K-SIGMA Threshold : ---thr 0.1908\n",
      "Counter({1: 65565, 0: 59867}) Counter({0: 89417, 1: 36015})\n",
      "Counts of  labels: {0: 89417, 1: 35978, 2: 1, 3: 1, 4: 1, 5: 2, 6: 31, 7: 1}\n",
      "Counts of misclassified original labels: {0: 29715, 1: 165}\n",
      "Test : Accuracy: 0.7618 Recall : 0.9954 FDR: 0.4532  F1-score: 0.7058  \n",
      "model AE is saved in best_models/AE_f1_0.71_recall_1.00_.pth\n",
      "Train : time 174.78 s Epoch 2\n",
      "Train Loss: 0.0026\n",
      "--- Running Evaluation for Epoch 2 lr =5e-05 wd 0.0001 ---\n",
      "-----------mse_loss mean :  0.0026 std: 0.0561\n",
      "Val: Accuracy: 0.9753  \n",
      " K: 1 K-SIGMA Threshold : ---thr 0.05864\n",
      "Counter({1: 65711, 0: 59721}) Counter({0: 89417, 1: 36015})\n",
      "Counts of  labels: {0: 89417, 1: 35978, 2: 1, 3: 1, 4: 1, 5: 2, 6: 31, 7: 1}\n",
      "Counts of misclassified original labels: {0: 29812, 1: 116}\n",
      "Test : Accuracy: 0.7614 Recall : 0.9968 FDR: 0.4537  F1-score: 0.7058  \n",
      " K: 3 K-SIGMA Threshold : ---thr 0.1708\n",
      "Counter({1: 65561, 0: 59871}) Counter({0: 89417, 1: 36015})\n",
      "Counts of  labels: {0: 89417, 1: 35978, 2: 1, 3: 1, 4: 1, 5: 2, 6: 31, 7: 1}\n",
      "Counts of misclassified original labels: {0: 29711, 1: 165}\n",
      "Test : Accuracy: 0.7618 Recall : 0.9954 FDR: 0.4532  F1-score: 0.7059  \n",
      "model AE is saved in best_models/AE_f1_0.71_recall_1.00_.pth\n",
      "Train : time 171.34 s Epoch 3\n",
      "Train Loss: 0.0025\n",
      "--- Running Evaluation for Epoch 3 lr =5e-05 wd 0.0001 ---\n",
      "-----------mse_loss mean :  0.0025 std: 0.0556\n",
      "Val: Accuracy: 0.9753  \n",
      " K: 1 K-SIGMA Threshold : ---thr 0.05811\n",
      "Counter({1: 65710, 0: 59722}) Counter({0: 89417, 1: 36015})\n",
      "Counts of  labels: {0: 89417, 1: 35978, 2: 1, 3: 1, 4: 1, 5: 2, 6: 31, 7: 1}\n",
      "Counts of misclassified original labels: {0: 29811, 1: 116}\n",
      "Test : Accuracy: 0.7614 Recall : 0.9968 FDR: 0.4537  F1-score: 0.7058  \n",
      " K: 3 K-SIGMA Threshold : ---thr 0.1694\n",
      "Counter({1: 65558, 0: 59874}) Counter({0: 89417, 1: 36015})\n",
      "Counts of  labels: {0: 89417, 1: 35978, 2: 1, 3: 1, 4: 1, 5: 2, 6: 31, 7: 1}\n",
      "Counts of misclassified original labels: {0: 29708, 1: 165}\n",
      "Test : Accuracy: 0.7618 Recall : 0.9954 FDR: 0.4532  F1-score: 0.7059  \n",
      "model AE is saved in best_models/AE_f1_0.71_recall_1.00_.pth\n",
      "Train : time 177.66 s Epoch 4\n",
      "Train Loss: 0.0024\n",
      "--- Running Evaluation for Epoch 4 lr =5e-05 wd 0.0001 ---\n",
      "-----------mse_loss mean :  0.0024 std: 0.0552\n",
      "Val: Accuracy: 0.9754  \n",
      " K: 1 K-SIGMA Threshold : ---thr 0.05764\n",
      "Counter({1: 65712, 0: 59720}) Counter({0: 89417, 1: 36015})\n",
      "Counts of  labels: {0: 89417, 1: 35978, 2: 1, 3: 1, 4: 1, 5: 2, 6: 31, 7: 1}\n",
      "Counts of misclassified original labels: {0: 29813, 1: 116}\n",
      "Test : Accuracy: 0.7614 Recall : 0.9968 FDR: 0.4537  F1-score: 0.7058  \n",
      " K: 3 K-SIGMA Threshold : ---thr 0.1681\n",
      "Counter({1: 65554, 0: 59878}) Counter({0: 89417, 1: 36015})\n",
      "Counts of  labels: {0: 89417, 1: 35978, 2: 1, 3: 1, 4: 1, 5: 2, 6: 31, 7: 1}\n",
      "Counts of misclassified original labels: {0: 29704, 1: 165}\n",
      "Test : Accuracy: 0.7619 Recall : 0.9954 FDR: 0.4531  F1-score: 0.7059  \n",
      "model AE is saved in best_models/AE_f1_0.71_recall_1.00_.pth\n",
      "Train : time 171.83 s Epoch 5\n",
      "Train Loss: 0.0024\n",
      "--- Running Evaluation for Epoch 5 lr =5e-05 wd 0.0001 ---\n",
      "-----------mse_loss mean :  0.0024 std: 0.0551\n",
      "Val: Accuracy: 0.9755  \n",
      " K: 1 K-SIGMA Threshold : ---thr 0.05744\n",
      "Counter({1: 65711, 0: 59721}) Counter({0: 89417, 1: 36015})\n",
      "Counts of  labels: {0: 89417, 1: 35978, 2: 1, 3: 1, 4: 1, 5: 2, 6: 31, 7: 1}\n",
      "Counts of misclassified original labels: {0: 29812, 1: 116}\n",
      "Test : Accuracy: 0.7614 Recall : 0.9968 FDR: 0.4537  F1-score: 0.7058  \n",
      " K: 3 K-SIGMA Threshold : ---thr 0.1676\n",
      "Counter({1: 65552, 0: 59880}) Counter({0: 89417, 1: 36015})\n",
      "Counts of  labels: {0: 89417, 1: 35978, 2: 1, 3: 1, 4: 1, 5: 2, 6: 31, 7: 1}\n",
      "Counts of misclassified original labels: {0: 29702, 1: 165}\n",
      "Test : Accuracy: 0.7619 Recall : 0.9954 FDR: 0.4531  F1-score: 0.7059  \n",
      "model AE is saved in best_models/AE_f1_0.71_recall_1.00_.pth\n",
      "Train : time 182.10 s Epoch 6\n",
      "Train Loss: 0.0024\n",
      "--- Running Evaluation for Epoch 6 lr =5e-05 wd 0.0001 ---\n"
     ]
    }
   ],
   "source": [
    "# train_eval(AE_model,AE_train_dataloader,AE_val_dataloader,AE_test_dataloader,shuffle_files=True,num_epochs=20,eval_epoch=1,criterion_method=\"bce\",learning_rates=[5e-4],weight_decays=[0])\n",
    "AE_model = AE(input_dim=76)\n",
    "train_eval(AE_model,train_dataloader,val_dataloader,test_dataloader,shuffle_files=False,num_epochs=6,eval_epoch=1,criterion_method=\"mse\",learning_rates=[5e-5,1e-4,1e-5,1e-6],weight_decays=[1e-4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Variational AutoEncoder (VAE)\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder: (89-64-64-32 for mu and log_var)\n",
    "    Decoder: (32-64-64-89)\n",
    "    return x_recon, mu, logvar\n",
    "    \"\"\"\n",
    "    def __init__(self,input_dim=89):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64, 32)\n",
    "        self.fc_logvar = nn.Linear(64, 32)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Sigmoid()\n",
    "                    )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================  lr=0.0005, wd=0 ==================\n",
      "Train : time 31.80 s Epoch 1\n",
      "Train Loss: 0.0971\n",
      "--- Running Evaluation for Epoch 1 lr =0.0005 wd 0 ---\n",
      "-----------mse_loss mean :  5.7442 std: 1.6715\n",
      "Val: Accuracy: 0.9991  \n",
      "---thr 74.16\n",
      "Counter({0: 246158, 1: 28}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 28, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Test : Accuracy: 0.8510 Recall : 0.0000 FDR: 1.0000  F1-score: 0.0000  \n",
      "---thr 37.08\n",
      "Counter({0: 246044, 1: 142}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 141, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 38, 6: 31, 7: 24}\n",
      "Test : Accuracy: 0.8505 Recall : 0.0000 FDR: 0.9930  F1-score: 0.0001  \n",
      "---thr 14.83\n",
      "Counter({0: 178037, 1: 68149}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 35848, 1: 4297, 2: 10, 3: 9, 4: 4, 5: 1, 6: 8, 7: 24}\n",
      "Test : Accuracy: 0.8367 Recall : 0.8812 FDR: 0.5260  F1-score: 0.6164  \n",
      "---thr 7.416\n",
      "Counter({0: 178023, 1: 68163}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 35857, 1: 4297, 2: 10, 3: 9, 6: 8, 7: 24}\n",
      "Test : Accuracy: 0.8367 Recall : 0.8814 FDR: 0.5260  F1-score: 0.6164  \n",
      "Train : time 31.31 s Epoch 2\n",
      "Train Loss: 0.0071\n",
      "--- Running Evaluation for Epoch 2 lr =0.0005 wd 0 ---\n",
      "-----------mse_loss mean :  5.7315 std: 1.5443\n",
      "Val: Accuracy: 0.9991  \n",
      "---thr 72.76\n",
      "Counter({0: 246176, 1: 10}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 10, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Test : Accuracy: 0.8511 Recall : 0.0000 FDR: 1.0000  F1-score: 0.0000  \n",
      "---thr 36.38\n",
      "Counter({0: 246042, 1: 144}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 142, 1: 36431, 2: 49, 3: 30, 4: 49, 5: 38, 6: 31, 7: 24}\n",
      "Test : Accuracy: 0.8505 Recall : 0.0001 FDR: 0.9861  F1-score: 0.0001  \n",
      "---thr 14.55\n",
      "Counter({0: 178039, 1: 68147}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 35847, 1: 4297, 2: 10, 3: 9, 4: 4, 5: 2, 6: 8, 7: 24}\n",
      "Test : Accuracy: 0.8367 Recall : 0.8812 FDR: 0.5260  F1-score: 0.6164  \n",
      "---thr 7.276\n",
      "Counter({0: 178023, 1: 68163}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 35857, 1: 4297, 2: 10, 3: 9, 6: 8, 7: 24}\n",
      "Test : Accuracy: 0.8367 Recall : 0.8814 FDR: 0.5260  F1-score: 0.6164  \n",
      "Train : time 31.71 s Epoch 3\n",
      "Train Loss: 0.0073\n",
      "--- Running Evaluation for Epoch 3 lr =0.0005 wd 0 ---\n",
      "-----------mse_loss mean :  5.7286 std: 1.5042\n",
      "Val: Accuracy: 0.9991  \n",
      "---thr 72.33\n",
      "Counter({0: 246186}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Test : Accuracy: 0.8511 Recall : 0.0000 FDR: 0.0000  F1-score: 0.0000  \n",
      "---thr 36.16\n",
      "Counter({0: 246012, 1: 174}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 163, 1: 36431, 2: 49, 3: 30, 4: 49, 5: 29, 6: 31, 7: 24}\n",
      "Test : Accuracy: 0.8505 Recall : 0.0003 FDR: 0.9368  F1-score: 0.0006  \n",
      "---thr 14.47\n",
      "Counter({0: 178038, 1: 68148}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 35847, 1: 4297, 2: 10, 3: 9, 4: 4, 5: 1, 6: 8, 7: 24}\n",
      "Test : Accuracy: 0.8367 Recall : 0.8812 FDR: 0.5260  F1-score: 0.6164  \n",
      "---thr 7.233\n",
      "Counter({0: 178005, 1: 68181}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 35874, 1: 4296, 2: 10, 3: 9, 6: 8, 7: 24}\n",
      "Test : Accuracy: 0.8366 Recall : 0.8814 FDR: 0.5262  F1-score: 0.6163  \n",
      "Train : time 30.10 s Epoch 4\n",
      "Train Loss: 0.0069\n",
      "--- Running Evaluation for Epoch 4 lr =0.0005 wd 0 ---\n",
      "-----------mse_loss mean :  5.7285 std: 1.5391\n",
      "Val: Accuracy: 0.9991  \n",
      "---thr 72.68\n",
      "Counter({0: 246186}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Test : Accuracy: 0.8511 Recall : 0.0000 FDR: 0.0000  F1-score: 0.0000  \n",
      "---thr 36.34\n",
      "Counter({0: 245998, 1: 188}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 172, 1: 36431, 2: 49, 3: 30, 4: 49, 5: 24, 6: 31, 7: 24}\n",
      "Test : Accuracy: 0.8505 Recall : 0.0004 FDR: 0.9149  F1-score: 0.0009  \n",
      "---thr 14.54\n",
      "Counter({0: 178039, 1: 68147}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 35847, 1: 4297, 2: 10, 3: 9, 4: 4, 5: 2, 6: 8, 7: 24}\n",
      "Test : Accuracy: 0.8367 Recall : 0.8812 FDR: 0.5260  F1-score: 0.6164  \n",
      "---thr 7.268\n",
      "Counter({0: 177997, 1: 68189}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 35882, 1: 4296, 2: 10, 3: 9, 6: 8, 7: 24}\n",
      "Test : Accuracy: 0.8366 Recall : 0.8814 FDR: 0.5262  F1-score: 0.6163  \n",
      "Train : time 29.64 s Epoch 5\n",
      "Train Loss: 0.0072\n",
      "--- Running Evaluation for Epoch 5 lr =0.0005 wd 0 ---\n",
      "-----------mse_loss mean :  5.7285 std: 1.5151\n",
      "Val: Accuracy: 0.9991  \n",
      "---thr 72.44\n",
      "Counter({0: 246186}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Test : Accuracy: 0.8511 Recall : 0.0000 FDR: 0.0000  F1-score: 0.0000  \n",
      "---thr 36.22\n",
      "Counter({0: 245977, 1: 209}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 175, 1: 36431, 2: 49, 3: 30, 4: 49, 5: 6, 6: 31, 7: 24}\n",
      "Test : Accuracy: 0.8505 Recall : 0.0009 FDR: 0.8373  F1-score: 0.0018  \n",
      "---thr 14.49\n",
      "Counter({0: 178039, 1: 68147}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 35847, 1: 4297, 2: 10, 3: 9, 4: 4, 5: 2, 6: 8, 7: 24}\n",
      "Test : Accuracy: 0.8367 Recall : 0.8812 FDR: 0.5260  F1-score: 0.6164  \n",
      "---thr 7.244\n",
      "Counter({0: 177993, 1: 68193}) Counter({0: 209532, 1: 36654})\n",
      "Counts of  labels: {0: 209532, 1: 36432, 2: 49, 3: 30, 4: 49, 5: 39, 6: 31, 7: 24}\n",
      "Counts of misclassified original labels: {0: 35886, 1: 4296, 2: 10, 3: 9, 6: 8, 7: 24}\n",
      "Test : Accuracy: 0.8366 Recall : 0.8814 FDR: 0.5262  F1-score: 0.6163  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m VAE_model \u001b[38;5;241m=\u001b[39m VAE(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m76\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVAE_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mAE_train_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mAE_val_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mAE_test_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshuffle_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43meval_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcriterion_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbce\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlearning_rates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5e-4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweight_decays\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 32\u001b[0m, in \u001b[0;36mtrain_eval\u001b[0;34m(model, train_dataloader, val_dataloader, test_dataloader, learning_rates, weight_decays, shuffle_files, num_epochs, eval_epoch, criterion_method)\u001b[0m\n\u001b[1;32m     30\u001b[0m     sys_rand\u001b[38;5;241m.\u001b[39mshuffle(train_dataloader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mcsv_files)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sequences, _ \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m---> 32\u001b[0m     sequences\u001b[38;5;241m=\u001b[39m\u001b[43msequences\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_get_name()\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdversarialAutoencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     34\u001b[0m         target_ones\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(sequences\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m,device\u001b[38;5;241m=\u001b[39mdevice,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "VAE_model = VAE(input_dim=76)\n",
    "train_eval(VAE_model,train_dataloader,val_dataloader,test_dataloader,shuffle_files=False,num_epochs=6,eval_epoch=1,criterion_method=\"mse\",learning_rates=[1e-2,1e-3,1e-4],weight_decays=[1e-4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a5bd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AAE_Encoder(nn.Module):\n",
    "    def __init__(self,input_dim=76):\n",
    "        \"\"\"\n",
    "        Encoder(Generator):(89-16-4-2)\n",
    "        \"\"\"\n",
    "        super(AAE_Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(16, 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(4, 2))\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "class AAE_Decoder(nn.Module):\n",
    "    def __init__(self,input_dim=76):\n",
    "        super(AAE_Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(4, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "class AAE_Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AAE_Discriminator, self).__init__()\n",
    "        # corrected to 2-16-4-1\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(4, 1), \n",
    "            nn.Sigmoid()\n",
    "        )    \n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n",
    " \n",
    "class AdversarialAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdversarialAutoencoder, self).__init__()\n",
    "        self.encoder = AAE_Encoder()\n",
    "        self.decoder = AAE_Decoder()\n",
    "        self.discriminator = AAE_Discriminator()\n",
    "    def forward(self, x):\n",
    "        fake_z = self.encoder(x)\n",
    "        x_recon = self.decoder(fake_z)\n",
    "        return fake_z,x_recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f371a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device (Method 1): cuda:0\n",
      "cuda:0\n",
      "AdversarialAutoencoder\n",
      "\n",
      "================== Evaluate lr=0.01, wd=0.0001 ==================\n",
      "Train : time 253.4021 Epoch 0\n",
      "Generator Loss: 0.8527 Discriminator Loss: 61.5562\n",
      "\n",
      "--- Running Evaluation for Epoch 1 lr =0.01 wd 0.0001 ---\n",
      "Computed Threshold: 0.0011\n",
      "Val: Accuracy: 0.9964  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "Train : time 228.5464 Epoch 1\n",
      "Generator Loss: 0.5067 Discriminator Loss: 60.1043\n",
      "\n",
      "--- Running Evaluation for Epoch 2 lr =0.01 wd 0.0001 ---\n",
      "Computed Threshold: 0.0011\n",
      "Val: Accuracy: 0.9939  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7682  \n",
      "\n",
      "================== Evaluate lr=0.01, wd=1e-05 ==================\n",
      "Train : time 210.8485 Epoch 0\n",
      "Generator Loss: 1.1983 Discriminator Loss: 299.7112\n",
      "\n",
      "--- Running Evaluation for Epoch 1 lr =0.01 wd 1e-05 ---\n",
      "Computed Threshold: 0.0014\n",
      "Val: Accuracy: 0.9957  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "Train : time 207.1711 Epoch 1\n",
      "Generator Loss: 0.5891 Discriminator Loss: 57.8186\n",
      "\n",
      "--- Running Evaluation for Epoch 2 lr =0.01 wd 1e-05 ---\n",
      "Computed Threshold: 0.0012\n",
      "Val: Accuracy: 0.9948  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "\n",
      "================== Evaluate lr=0.01, wd=1e-06 ==================\n",
      "Train : time 222.9285 Epoch 0\n",
      "Generator Loss: 1.1046 Discriminator Loss: 46.7173\n",
      "\n",
      "--- Running Evaluation for Epoch 1 lr =0.01 wd 1e-06 ---\n",
      "Computed Threshold: 0.0020\n",
      "Val: Accuracy: 0.9969  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "Train : time 236.7508 Epoch 1\n",
      "Generator Loss: 1.0842 Discriminator Loss: 60.5046\n",
      "\n",
      "--- Running Evaluation for Epoch 2 lr =0.01 wd 1e-06 ---\n",
      "Computed Threshold: 0.0023\n",
      "Val: Accuracy: 0.9931  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "\n",
      "================== Evaluate lr=0.001, wd=0.0001 ==================\n",
      "Train : time 232.1753 Epoch 0\n",
      "Generator Loss: 3.4004 Discriminator Loss: 48.3463\n",
      "\n",
      "--- Running Evaluation for Epoch 1 lr =0.001 wd 0.0001 ---\n",
      "Computed Threshold: 0.0011\n",
      "Val: Accuracy: 0.9966  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "Train : time 242.4341 Epoch 1\n",
      "Generator Loss: 0.4077 Discriminator Loss: 50.4980\n",
      "\n",
      "--- Running Evaluation for Epoch 2 lr =0.001 wd 0.0001 ---\n",
      "Computed Threshold: 0.0010\n",
      "Val: Accuracy: 0.9967  \n",
      "Test : Accuracy: 0.8897 Recall : 0.8443 FDR: 0.2943  F1-score: 0.7688  \n",
      "\n",
      "================== Evaluate lr=0.001, wd=1e-05 ==================\n",
      "Train : time 221.1261 Epoch 0\n",
      "Generator Loss: 1.6774 Discriminator Loss: 40.1020\n",
      "\n",
      "--- Running Evaluation for Epoch 1 lr =0.001 wd 1e-05 ---\n",
      "Computed Threshold: 0.0018\n",
      "Val: Accuracy: 0.9967  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "Train : time 239.5897 Epoch 1\n",
      "Generator Loss: 0.5316 Discriminator Loss: 53.9577\n",
      "\n",
      "--- Running Evaluation for Epoch 2 lr =0.001 wd 1e-05 ---\n",
      "Computed Threshold: 0.0008\n",
      "Val: Accuracy: 0.9969  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8442 FDR: 0.2954  F1-score: 0.7681  \n",
      "\n",
      "================== Evaluate lr=0.001, wd=1e-06 ==================\n",
      "Train : time 209.5135 Epoch 0\n",
      "Generator Loss: 2.9068 Discriminator Loss: 39.0097\n",
      "\n",
      "--- Running Evaluation for Epoch 1 lr =0.001 wd 1e-06 ---\n",
      "Computed Threshold: 0.0012\n",
      "Val: Accuracy: 0.9964  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "Train : time 259.2067 Epoch 1\n",
      "Generator Loss: 0.3805 Discriminator Loss: 52.0293\n",
      "\n",
      "--- Running Evaluation for Epoch 2 lr =0.001 wd 1e-06 ---\n",
      "Computed Threshold: 0.0009\n",
      "Val: Accuracy: 0.9964  \n",
      "Test : Accuracy: 0.8897 Recall : 0.8442 FDR: 0.2943  F1-score: 0.7688  \n",
      "\n",
      "================== Evaluate lr=0.0001, wd=0.0001 ==================\n",
      "Train : time 217.9773 Epoch 0\n",
      "Generator Loss: 29.6505 Discriminator Loss: 26.5341\n",
      "\n",
      "--- Running Evaluation for Epoch 1 lr =0.0001 wd 0.0001 ---\n",
      "Computed Threshold: 0.0024\n",
      "Val: Accuracy: 0.9939  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "Train : time 234.4891 Epoch 1\n",
      "Generator Loss: 1.7832 Discriminator Loss: 44.4554\n",
      "\n",
      "--- Running Evaluation for Epoch 2 lr =0.0001 wd 0.0001 ---\n",
      "Computed Threshold: 0.0016\n",
      "Val: Accuracy: 0.9950  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "\n",
      "================== Evaluate lr=0.0001, wd=1e-05 ==================\n",
      "Train : time 232.3771 Epoch 0\n",
      "Generator Loss: 13.3208 Discriminator Loss: 45.0669\n",
      "\n",
      "--- Running Evaluation for Epoch 1 lr =0.0001 wd 1e-05 ---\n",
      "Computed Threshold: 0.0015\n",
      "Val: Accuracy: 0.9920  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "Train : time 237.0694 Epoch 1\n",
      "Generator Loss: 0.5830 Discriminator Loss: 30.6276\n",
      "\n",
      "--- Running Evaluation for Epoch 2 lr =0.0001 wd 1e-05 ---\n",
      "Computed Threshold: 0.0020\n",
      "Val: Accuracy: 0.9970  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "\n",
      "================== Evaluate lr=0.0001, wd=1e-06 ==================\n",
      "Train : time 255.5520 Epoch 0\n",
      "Generator Loss: 8.3159 Discriminator Loss: 33.0867\n",
      "\n",
      "--- Running Evaluation for Epoch 1 lr =0.0001 wd 1e-06 ---\n",
      "Computed Threshold: 0.0018\n",
      "Val: Accuracy: 0.9939  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "Train : time 232.1961 Epoch 1\n",
      "Generator Loss: 0.8245 Discriminator Loss: 30.6750\n",
      "\n",
      "--- Running Evaluation for Epoch 2 lr =0.0001 wd 1e-06 ---\n",
      "Computed Threshold: 0.0020\n",
      "Val: Accuracy: 0.9939  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "\n",
      "================== Evaluate lr=1e-05, wd=0.0001 ==================\n",
      "Train : time 232.0385 Epoch 0\n",
      "Generator Loss: 96.4000 Discriminator Loss: 34.8106\n",
      "\n",
      "--- Running Evaluation for Epoch 1 lr =1e-05 wd 0.0001 ---\n",
      "Computed Threshold: 0.0055\n",
      "Val: Accuracy: 0.9916  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "Train : time 229.6131 Epoch 1\n",
      "Generator Loss: 5.2372 Discriminator Loss: 9.0938\n",
      "\n",
      "--- Running Evaluation for Epoch 2 lr =1e-05 wd 0.0001 ---\n",
      "Computed Threshold: 0.0047\n",
      "Val: Accuracy: 0.9940  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "\n",
      "================== Evaluate lr=1e-05, wd=1e-05 ==================\n",
      "Train : time 257.6845 Epoch 0\n",
      "Generator Loss: 98.1880 Discriminator Loss: 64.7304\n",
      "\n",
      "--- Running Evaluation for Epoch 1 lr =1e-05 wd 1e-05 ---\n",
      "Computed Threshold: 0.0104\n",
      "Val: Accuracy: 0.9076  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "Train : time 196.7555 Epoch 1\n",
      "Generator Loss: 9.3170 Discriminator Loss: 29.8478\n",
      "\n",
      "--- Running Evaluation for Epoch 2 lr =1e-05 wd 1e-05 ---\n",
      "Computed Threshold: 0.0047\n",
      "Val: Accuracy: 0.9913  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "\n",
      "================== Evaluate lr=1e-05, wd=1e-06 ==================\n",
      "Train : time 196.0246 Epoch 0\n",
      "Generator Loss: 111.8188 Discriminator Loss: 51.5895\n",
      "\n",
      "--- Running Evaluation for Epoch 1 lr =1e-05 wd 1e-06 ---\n",
      "Computed Threshold: 0.0062\n",
      "Val: Accuracy: 0.9913  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n",
      "Train : time 194.2053 Epoch 1\n",
      "Generator Loss: 4.6856 Discriminator Loss: 14.4423\n",
      "\n",
      "--- Running Evaluation for Epoch 2 lr =1e-05 wd 1e-06 ---\n",
      "Computed Threshold: 0.0052\n",
      "Val: Accuracy: 0.9940  \n",
      "Test : Accuracy: 0.8894 Recall : 0.8443 FDR: 0.2954  F1-score: 0.7681  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "aae_model = AdversarialAutoencoder().to(device)\n",
    "device = next(aae_model.parameters()).device\n",
    "print(f\"Model device (Method 1): {device}\")\n",
    "print(next(aae_model.decoder.parameters()).device)\n",
    "print(aae_model._get_name())\n",
    "train_eval(aae_model,AE_train_dataloader,AE_val_dataloader,AE_test_dataloader)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # aae_encoder.train()\n",
    "#     # aae_decoder.train()\n",
    "#     # aae_discriminator.train()\n",
    "#     aae_model.train()\n",
    "#     if shuffle_files:\n",
    "#         sys_rand = SystemRandom()\n",
    "#         sys_rand.shuffle(AE_dataset.file_order_indices)\n",
    "#     for sequences,_ in AE_dataloader:\n",
    "#         sequences=sequences.squeeze().to(device)\n",
    "#         # 1) reconstruction + generator loss\n",
    "#         optimizer_G.zero_grad()\n",
    "#         fake_z = aae_encoder(sequences)\n",
    "#         decoded_seq = aae_decoder(fake_z)\n",
    "#         G_loss = 0.001*adversarial_loss(aae_discriminator(fake_z),  torch.ones(sequences.size(0), 2,device=device)) + 0.999*reconstruction_loss(decoded_seq, sequences)\n",
    "#         G_loss.backward()\n",
    "#         optimizer_G.step()\n",
    "#         # 2) discriminator loss\n",
    "#         optimizer_D.zero_grad()\n",
    "#         real_loss = adversarial_loss(aae_discriminator(torch.randn(sequences.size(0), 2, device=device)),  torch.ones(sequences.size(0), 2,device=device))\n",
    "#         fake_loss = adversarial_loss(aae_discriminator(fake_z.detach()),  torch.zeros(sequences.size(0), 2,device=device))\n",
    "#         D_loss = 0.5*(real_loss + fake_loss)\n",
    "#         D_loss.backward()\n",
    "#         optimizer_D.step()\n",
    "#     # print loss\n",
    "#     print(\n",
    "#             \"[Epoch %d/%d] [G loss: %f] [D loss: %f]\"\n",
    "#             % (epoch, num_epochs, G_loss.item(), D_loss.item())\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54635226",
   "metadata": {},
   "outputs": [],
   "source": [
    "AAE_model = AdversarialAutoencoder()\n",
    "train_eval(AAE_model,train_dataloader,val_dataloader,test_dataloader,shuffle_files=False,num_epochs=6,eval_epoch=1,criterion_method=\"mse\",learning_rates=[1e-2,1e-3,1e-4],weight_decays=[1e-4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3698c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU-VAE\n",
    "class GRUVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Recurrent Unit : num_layers=2, hidden_size=256, dropout=0.01,window size (seq_len)= 40\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=89, hidden_dim=256, latent_dim=32, num_layers=2, dropout=0.01):\n",
    "        super(GRUVAE, self).__init__()\n",
    "        self.encoder_gru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_z_to_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_gru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, input_dim=89]\n",
    "        _, hidden = self.encoder_gru(x) \n",
    "        h = hidden[-1]  # [batch_size, hidden_dim]\n",
    "        mu = self.fc_mu(h)  \n",
    "        logvar = self.fc_logvar(h)  \n",
    "        z = self.reparameterize(mu, logvar)  # [batch_size, latent_dim]\n",
    "        # repeat and feed latent z as input trick\n",
    "        h0 = self.fc_z_to_hidden(z).unsqueeze(0).repeat(self.encoder_gru.num_layers, 1, 1)  # [num_layers, batch_size, hidden_dim]\n",
    "        # Initialize decoder input with zeros \n",
    "        decoder_input = torch.zeros_like(x)\n",
    "        output, _ = self.decoder_gru(decoder_input, h0)  # [batch_size, seq_len, hidden_dim]\n",
    "        x_recon = self.fc_out(output)  # [batch_size, seq_len, input_dim]\n",
    "        return nn.Sigmoid(x_recon), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e51d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded scalers for 'network-wide'\n"
     ]
    }
   ],
   "source": [
    "loaded_scalers = load_scalers(\"fitted_scalers\")\n",
    "RNN_train_dataset=ModbusFlowStream( \n",
    "    shuffle=False,chunk_size=1,batch_size=64,csv_files=train_files,\n",
    "    scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=10\n",
    ")\n",
    "RNN_train_dataloader=DataLoader(RNN_train_dataset,batch_size=1,shuffle=False)\\\n",
    "\n",
    "RNN_val_dataset=ModbusFlowStream( \n",
    "    shuffle=False,chunk_size=1,batch_size=64,csv_files=val_files,\n",
    "    scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=10\n",
    ")\n",
    "RNN_val_dataloader=DataLoader(RNN_val_dataset,batch_size=1,shuffle=False)\n",
    "\n",
    "\n",
    "RNN_test_dataset=ModbusFlowStream( \n",
    "    shuffle=False,chunk_size=1,batch_size=64,csv_files=test_files,\n",
    "    scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=10\n",
    ")\n",
    "RNN_test_dataloader=DataLoader(RNN_test_dataset,batch_size=1,shuffle=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee7735e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================  lr=5e-05, wd=1e-05 ==================\n",
      "Train : time 219.29 s Epoch 1\n",
      "Train Loss: 4.8276\n",
      "--- Running Evaluation for Epoch 1 lr =5e-05 wd 1e-05 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid_rd3/labeling/Project1404/FLBased-ICS-NIDS/vnv/lib/python3.10/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([9, 30, 89])) that is different to the input size (torch.Size([64, 30, 89])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (9) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGRU_VAE_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mRNN_train_dataloadder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mRNN_test_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mRNN_test_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 79\u001b[0m, in \u001b[0;36mtrain_eval\u001b[0;34m(model, train_dataloader, val_dataloader, test_dataloader, learning_rates, weight_decays, shuffle_files, num_epochs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_get_name()\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdversarialAutoencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     78\u001b[0m     _,recon\u001b[38;5;241m=\u001b[39m model(sequences)\n\u001b[0;32m---> 79\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43meval_criterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     81\u001b[0m     intrusion_scores \u001b[38;5;241m=\u001b[39m val_loss\n",
      "File \u001b[0;32m~/labeling/Project1404/FLBased-ICS-NIDS/vnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/labeling/Project1404/FLBased-ICS-NIDS/vnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/labeling/Project1404/FLBased-ICS-NIDS/vnv/lib/python3.10/site-packages/torch/nn/modules/loss.py:610\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/labeling/Project1404/FLBased-ICS-NIDS/vnv/lib/python3.10/site-packages/torch/nn/functional.py:3884\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3882\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3884\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3887\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n",
      "File \u001b[0;32m~/labeling/Project1404/FLBased-ICS-NIDS/vnv/lib/python3.10/site-packages/torch/functional.py:77\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (9) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "GRU_VAE_model = GRUVAE()\n",
    "train_eval(GRU_VAE_model,RNN_train_dataloader,RNN_val_dataloader,RNN_test_dataloader,shuffle_files=False,num_epochs=6,eval_epoch=1,criterion_method=\"mse\",learning_rates=[1e-2,1e-3,1e-4],weight_decays=[1e-4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 103.30588173866272 Epoch 0, Train Loss: 127700652.60516566\n",
      "time 95.60702395439148 Epoch 1, Train Loss: 70232281.69612122\n",
      "time 94.19640827178955 Epoch 2, Train Loss: 3457994.1813964844\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    time_1 = time.time()\n",
    "    train_loss = 0\n",
    "    GRU_VAE_model.train()\n",
    "    if shuffle_files:\n",
    "        sys_rand = SystemRandom()\n",
    "        sys_rand.shuffle(RNN_dataset.file_order_indices)\n",
    "    for sequences, _ in RNN_dataloadder:\n",
    "        sequences = sequences.squeeze().to(device)\n",
    "        GRU_VAE_optimizer.zero_grad()\n",
    "        recon, mu, logvar = GRU_VAE_model(sequences)\n",
    "        loss = vae_loss_function(recon, sequences, mu, logvar)/sequences.size(0)\n",
    "        loss.backward()\n",
    "        GRU_VAE_optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(\"time\",time.time()-time_1,f\"Epoch {epoch}, Train Loss: {train_loss/len(RNN_dataloadder)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e10197",
   "metadata": {},
   "source": [
    "### Federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f948763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. IMPORT DEPENDENCIES\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warning messages for a cleaner output\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "# Set a seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {DEVICE}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CONFIGURATION: TWEAK YOUR FEDERATED LEARNING EXPERIMENT\n",
    "# ==============================================================================\n",
    "class Config:\n",
    "    \"\"\"Configuration class for the federated learning experiment.\"\"\"\n",
    "    NUM_CLIENTS = 5\n",
    "    NUM_ROUNDS = 10\n",
    "    LOCAL_EPOCHS = 5\n",
    "    BATCH_SIZE = 32\n",
    "    lr = 0.001\n",
    "    wd = 1e-4\n",
    "    # Choose from \"FED_AVG\", \"FED_PROX\"\n",
    "    STRATEGY = \"FED_AVG\"\n",
    "    # Proximal term for FedProx, only used if STRATEGY is \"FED_PROX\"\n",
    "    PROXIMAL_MU = 0.1\n",
    "\n",
    "# Instantiate the configuration\n",
    "cfg = Config()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. DATA PREPARATION: DUMMY DATA AND CUSTOM DATASET\n",
    "# ==============================================================================\n",
    "\n",
    "ied1b_train_files=[col for col in modbus.dataset[\"benign_dataset_dir\"] if col.find(\"ied1b\")!=-1]\n",
    "ied1a_train_files=[col for col in modbus.dataset[\"benign_dataset_dir\"] if col.find(\"ied1a\")!=-1]\n",
    "ied4c_train_files=[col for col in modbus.dataset[\"benign_dataset_dir\"] if col.find(\"ied4c\")!=-1]\n",
    "scada_train_files=[col for col in modbus.dataset[\"benign_dataset_dir\"] if col.find(\"scada-hmi\")!=-1]\n",
    "cent_agent_train_files=[col for col in modbus.dataset[\"benign_dataset_dir\"] if col.find(\"central-agent\")!=-1]\n",
    "\n",
    "CLIENT_DATA_MAPPING = {\n",
    "    0: [ied1b_train_files],\n",
    "    1: [ied1a_train_files],\n",
    "    2: [ied4c_train_files],\n",
    "    3: [scada_train_files],\n",
    "    4: [cent_agent_train_files],\n",
    "    5:[cent_agent_train_files,cent_agent_train_files],#eval-test\n",
    "}\n",
    "\n",
    "# --- Data Loading Function for Clients ---\n",
    "def load_data(client_id: int, model_name: str):\n",
    "    \"\"\"Loads the data for a specific client based on the mapping.\"\"\"\n",
    "    file_list = CLIENT_DATA_MAPPING[client_id]\n",
    "    if len(file_list)==1:#train\n",
    "\n",
    "    if model_name!=\"GRUVAE\":\n",
    "        train_dataset=ModbusFlowStream( \n",
    "        shuffle=True,chunk_size=20,batch_size=64,csv_files=file_list,scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=1)\n",
    "    else: #\"AE\" , \"VAE\" , \"AAE\"    \n",
    "        train_dataset=ModbusFlowStream( \n",
    "        shuffle=False,chunk_size=20,batch_size=64,csv_files=file_list,scalers=loaded_scalers['network-wide']['min_max_scalers'],window_size=30)\n",
    "    train_dataloader=DataLoader(train_dataset,batch_size=1,shuffle=False)\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. FEDERATED LEARNING CLIENT: FlowerClient\n",
    "# ==============================================================================\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    \"\"\"Flower client for training the AutoEncoder.\"\"\"\n",
    "    def __init__(self, cid, model, trainloader=None,val_loader=None,test_loader=None):\n",
    "        self.cid = cid\n",
    "        self.model = model\n",
    "        self.train_dataloader = trainloader\n",
    "        self.val_dataloader=val_loader\n",
    "         self.test_dataloader=test_data_loader\n",
    "    def get_parameters(self, config):\n",
    "        \"\"\"Return model parameters as a list of NumPy arrays.\"\"\"\n",
    "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        \"\"\"Update model parameters from a list of NumPy arrays.\"\"\"\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"Train the model on local data.\"\"\"\n",
    " \n",
    "        self.set_parameters(parameters)\n",
    "        # Add proximal term for FedProx\n",
    "        proximal_term = 0.\n",
    "        \n",
    "        if cfg.STRATEGY == \"FED_PROX\":\n",
    "            global_params = [torch.tensor(p, device=DEVICE) for p in parameters]\n",
    "    \n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = self.model\n",
    "        model.to(device)\n",
    "        criterion = nn.MSELoss(reduction='sum').to(device)\n",
    "        eval_criterion = nn.MSELoss(reduction='none').to(device)\n",
    "        if model._get_name()==\"AdversarialAutoencoder\":\n",
    "            adversarial_criterion= nn.BCELoss(reduction=\"sum\")\n",
    "            optimizer_D = optim.Adam(model.discriminator.parameters(), lr=cfg.lr, weight_decay=cfg.wd)\n",
    "            optimizer_G =  optim.Adam(list(model.encoder.parameters()) + list(model.decoder.parameters()), lr=cfg.lr, weight_decay=cfg.wd)\n",
    "        else:\n",
    "            AE_optimizer = optim.Adam(model.parameters(), lr=cfg.lr,weight_decay=cfg.wd)\n",
    "        print(f\"\\n==================client id={self.cid}  lr={self.lr}, wd={self.wd} ==================\")\n",
    "        model.apply(_init_weights)\n",
    "        for epoch in range(cfg.LOCAL_EPOCHS):\n",
    "            time_1 = time.time()\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            ## for AAE\n",
    "            Discriminator_loss = 0\n",
    "            if shuffle_files:\n",
    "                sys_rand = SystemRandom()\n",
    "                sys_rand.shuffle(self.train_dataloader.dataset.csv_files)\n",
    "            for sequences, _ in self.train_dataloader:\n",
    "                sequences=sequences.squeeze().to(device)\n",
    "                if model._get_name()==\"AdversarialAutoencoder\":\n",
    "                    target_ones= torch.ones(sequences.size(0), 1,device=device,dtype=torch.float)\n",
    "                    target_zeros= torch.zeros(sequences.size(0), 1,device=device,dtype=torch.float)\n",
    "                    random_latent = torch.randn(sequences.size(0), 2, device=device)\n",
    "                    optimizer_G.zero_grad()\n",
    "                    fake_z,decoded_seq = model(sequences)\n",
    "                    G_loss = 0.001*adversarial_criterion(model.discriminator(fake_z),target_ones ) + 0.999*criterion(decoded_seq, sequences)\n",
    "                    G_loss.backward()\n",
    "                    optimizer_G.step()\n",
    "                    # 2) discriminator loss\n",
    "                    optimizer_D.zero_grad()\n",
    "                    real_loss = adversarial_criterion(model.discriminator(random_latent), target_ones)\n",
    "                    fake_loss = adversarial_criterion(model.discriminator(fake_z.detach()),  target_zeros)\n",
    "                    D_loss =  0.001*0.5*(real_loss + fake_loss)\n",
    "                    D_loss.backward()\n",
    "                    optimizer_D.step()\n",
    "                    train_loss+=G_loss.item()\n",
    "                    Discriminator_loss+=D_loss.item()   \n",
    "                else:\n",
    "                    AE_optimizer.zero_grad()\n",
    "                    if model._get_name()==\"AE\":\n",
    "                        recon = model(sequences)\n",
    "                        loss = criterion(recon, sequences) / sequences.size(0)\n",
    "                    elif model._get_name()==\"VAE\" or model._get_name()==\"GRUVAE\":\n",
    "                        recon, mu, logvar = model(sequences)\n",
    "                        loss = vae_loss_function(recon, sequences, mu, logvar) /sequences.size(0)\n",
    "                    loss.backward()\n",
    "                    AE_optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "            print(f\"Train : time {(time.time()-time_1):.2f} s\",\n",
    "            f\"Epoch {epoch+1}\")\n",
    "            if model._get_name()==\"AdversarialAutoencoder\":\n",
    "                print(f\"Generator Loss: {train_loss / len(self.train_dataloader):.4f}\",\n",
    "                    f\"Discriminator Loss: {Discriminator_loss / len(self.train_dataloader):.4f}\")\n",
    "            else:\n",
    "                print(f\"client id {self.cid} Train Loss: {train_loss / len(self.train_dataloader):.4f}\")\n",
    "        \n",
    "        self.model = model\n",
    "        return self.get_parameters(config={}), len(self.train_dataloader.dataset), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        \"\"\"Evaluate the model on the local test set.\"\"\"\n",
    "        self.set_parameters(parameters)\n",
    "        model = self.model()\n",
    "        model.to(DEVICE)\n",
    "        # Evaluate part\n",
    "        if (epoch + 1) % eval_epoch == 0:\n",
    "            model.eval() \n",
    "            all_val_losses = []\n",
    "            all_val_labels = []\n",
    "            print(f\"--- Running Evaluation for Epoch {epoch+1} lr ={lr} wd {wd} ---\")\n",
    "            with torch.no_grad():\n",
    "                for sequences, labels in self.val_dataloader:\n",
    "                    sequences = sequences.squeeze().to(device)        \n",
    "                    if model._get_name()==\"AE\":\n",
    "                        recon = model(sequences)\n",
    "                    elif model._get_name()==\"VAE\" or model._get_name()==\"GRUVAE\" :\n",
    "                        recon, _, _ = model(sequences)\n",
    "                    elif model._get_name()==\"AdversarialAutoencoder\":\n",
    "                        _,recon= model(sequences)\n",
    "                    val_loss = eval_criterion(recon, sequences)\n",
    "                    if val_loss.dim() > 1:\n",
    "                        val_loss = val_loss\n",
    "                    else:\n",
    "                        val_loss = val_loss.unsqueeze(dim=0)\n",
    "                        labels = labels.unsqueeze(dim=0)\n",
    "                    val_loss = val_loss.sum(dim=1)\n",
    "                    all_val_losses.extend(val_loss.cpu().numpy())\n",
    "                    all_val_labels.extend(labels.flatten().cpu().numpy())            \n",
    "            threshold_1 = compute_threshold(all_val_losses,k=1)\n",
    "\n",
    "            all_val_losses = np.array(all_val_losses).squeeze()  \n",
    "            all_val_labels = np.array(all_val_labels).squeeze()  \n",
    "            # If intrusion score > threshold, predict 1 (intrusion), else 0 (benign)\n",
    "            # For FDR, get True Positives (TP) and False Positives (FP)\n",
    "            print(all_val_losses.shape,all_val_labels.shape,predictions.shape)\n",
    "            predictions = (all_val_losses > threshold_1).astype(int)\n",
    "            accuracy = accuracy_score(all_val_labels, predictions)\n",
    "            print(f\"Val: Accuracy: {accuracy:.4f}  \")\n",
    "            model.eval() \n",
    "            all_test_losses = []\n",
    "            all_test_labels = []\n",
    "            with torch.no_grad():\n",
    "                for sequences, labels in self.test_dataloader:\n",
    "                    sequences = sequences.squeeze().to(device)\n",
    "                    labels = labels.squeeze().to(device)\n",
    "                    if model._get_name()==\"AE\":\n",
    "                        recon = model(sequences)\n",
    "                    elif model._get_name()==\"VAE\"  or model._get_name()==\"GRUVAE\":\n",
    "                        recon, mu, logvar = model(sequences)\n",
    "                    elif model._get_name()==\"AdversarialAutoencoder\":\n",
    "                        _,recon= model(sequences)\n",
    "                    intrusion_scores = eval_criterion(recon, sequences)\n",
    "                    if intrusion_scores.dim() > 1:\n",
    "                        intrusion_scores = intrusion_scores\n",
    "                    else:\n",
    "                        intrusion_scores = intrusion_scores.unsqueeze(dim=0)\n",
    "                        labels = labels.unsqueeze(dim=0)\n",
    "                    intrusion_scores = intrusion_scores.sum(dim=1)\n",
    "                    all_test_losses.extend(intrusion_scores.cpu().numpy())\n",
    "                    all_test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            all_test_losses = np.array(all_test_losses)\n",
    "            all_test_labels = np.array(all_test_labels)\n",
    "            # for threshold in {threshold_1,threshold_2,threshold_3,threshold_10,threshold_100}:\n",
    "            for threshold in {threshold_1}:\n",
    "\n",
    "                print(f\"---thr {threshold:.4}\")\n",
    "                predictions = (all_test_losses > threshold).astype(int)\n",
    "                binary_test_labels = (all_test_labels != 0).astype(int)\n",
    "                # --- Start of new code ---\n",
    "\n",
    "                # Find the indices where the prediction was incorrect\n",
    "                misclassified_indices = np.where(binary_test_labels != predictions)[0]\n",
    "\n",
    "                # Get the original labels for those misclassified instances\n",
    "                misclassified_original_labels = all_test_labels[misclassified_indices]\n",
    "                # To get a summary count of which labels were misclassified\n",
    "                print(f\"Counts of  labels: {dict(sorted(Counter(all_test_labels).items()))}\")\n",
    "                print(f\"Counts of misclassified original labels: {dict(sorted(Counter(misclassified_original_labels).items()))}\")\n",
    "\n",
    "                accuracy = accuracy_score(binary_test_labels, predictions)\n",
    "                f1 = f1_score(binary_test_labels, predictions, zero_division=0)\n",
    "                recall = recall_score(binary_test_labels, predictions,zero_division=0)\n",
    "                _, fp, _, tp = confusion_matrix(binary_test_labels, predictions, labels=[0, 1]).ravel()\n",
    "                # FDR = FP / (FP + TP) \n",
    "                if (fp + tp) == 0:\n",
    "                    fdr = 0.0 \n",
    "                else:\n",
    "                    fdr = fp / (fp + tp)\n",
    "                print(f\"Test : Accuracy: {accuracy:.4f} Recall : {recall:.4f} FDR: {fdr:.4f}  F1-score: {f1:.4f} \")\n",
    "                return np.mean(all_test_losses), len(self.testloader.dataset), {\"accuracy\": accuracy,\"recall\":Recall,\"FDR\":FDR, \"f1_score\": f1}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. SERVER-SIDE LOGIC AND SIMULATION START\n",
    "# ==============================================================================\n",
    "def client_fn(cid: str) -> FlowerClient:\n",
    "    \"\"\"Create a Flower client instance for a given client ID.\"\"\"\n",
    "    client_id = int(cid)\n",
    "    # Load model and data for this client\n",
    "    model = AutoEncoder().to(DEVICE)\n",
    "    trainloader, testloader = load_data(client_id,model._get_name())\n",
    "    return FlowerClient(cid, model, trainloader, testloader).to_client()\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Dict[str, float]]]) -> Dict[str, float]:\n",
    "    \"\"\"Aggregate evaluation metrics from all clients.\"\"\"\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    f1_scores = [num_examples * m[\"f1_score\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metrics\n",
    "    return {\n",
    "        \"accuracy\": sum(accuracies) / sum(examples),\n",
    "        \"f1_score\": sum(f1_scores) / sum(examples),\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7758849,
     "sourceId": 12309500,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 250292947,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "vnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
